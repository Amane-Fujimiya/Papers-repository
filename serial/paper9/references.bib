@inproceedings{
Oono2020Graph,
title={Graph Neural Networks Exponentially Lose Expressive Power for Node Classification},
author={Kenta Oono and Taiji Suzuki},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=S1ldO2EFPr}
}

@book{10.5555/200548,
author = {Kearns, Michael J. and Vazirani, Umesh V.},
title = {An introduction to computational learning theory},
year = {1994},
isbn = {0262111934},
publisher = {MIT Press},
address = {Cambridge, MA, USA}
}
@article{brown2024biasvariance,
title={Bias/Variance is not the same as Approximation/Estimation},
author={Gavin Brown and Riccardo Ali},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=4TnFbv16hK},
note={}
}
@misc{PfauBregmanDivergence,
  author = {David Pfau},
  howpublished = {Technical report},
  title = {A Generalized Bias-Variance Decomposition for Bregman Divergences},
  year = {2013}
}
@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}
@article{10.1145/1968.1972,
author = {Valiant, L. G.},
title = {A theory of the learnable},
year = {1984},
issue_date = {Nov. 1984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/1968.1972},
doi = {10.1145/1968.1972},
journal = {Commun. ACM},
month = nov,
pages = {1134–1142},
numpages = {9},
keywords = {inductive inference, probabilistic models of learning, propositional expressions}
}
@article{Rosenblatt1958ThePA,
  title={The perceptron: a probabilistic model for information storage and organization in the brain.},
  author={Frank Rosenblatt},
  journal={Psychological review},
  year={1958},
  volume={65 6},
  pages={
          386-408
        },
  url={https://api.semanticscholar.org/CorpusID:12781225}
}
@misc{shi2024homophilymodulatesdoubledescent,
      title={Homophily modulates double descent generalization in graph convolution networks}, 
      author={Cheng Shi and Liming Pan and Hong Hu and Ivan Dokmanić},
      year={2024},
      eprint={2212.13069},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2212.13069}, 
}
@book{STL_Hajek_Maxim_2021,
title={Statistical Learning Theory},
author={Bruce Hajek and Maxim Raginsky},
volume={1},
year={2021},
url = {https://maxim.ece.illinois.edu/teaching/SLT/}
}
@misc{bousquet2020theoryuniversallearning,
      title={A Theory of Universal Learning}, 
      author={Olivier Bousquet and Steve Hanneke and Shay Moran and Ramon van Handel and Amir Yehudayoff},
      year={2020},
      eprint={2011.04483},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2011.04483}, 
}

@book{10.5555/2621980,
author = {Shalev-Shwartz, Shai and Ben-David, Shai},
title = {Understanding Machine Learning: From Theory to Algorithms},
year = {2014},
isbn = {1107057132},
publisher = {Cambridge University Press},
address = {USA},
abstract = {Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering.}
}

@inproceedings{sharma_bias-variance_2014,
	address = {New York, NY, USA},
	series = {{POPL} '14},
	title = {Bias-variance tradeoffs in program analysis},
	isbn = {978-1-4503-2544-8},
	url = {https://doi.org/10.1145/2535838.2535853},
	doi = {10.1145/2535838.2535853},
	abstract = {It is often the case that increasing the precision of a program analysis leads to worse results. It is our thesis that this phenomenon is the result of fundamental limits on the ability to use precise abstract domains as the basis for inferring strong invariants of programs. We show that bias-variance tradeoffs, an idea from learning theory, can be used to explain why more precise abstractions do not necessarily lead to better results and also provides practical techniques for coping with such limitations. Learning theory captures precision using a combinatorial quantity called the VC dimension. We compute the VC dimension for different abstractions and report on its usefulness as a precision metric for program analyses. We evaluate cross validation, a technique for addressing bias-variance tradeoffs, on an industrial strength program verification tool called YOGI. The tool produced using cross validation has significantly better running time, finds new defects, and has fewer time-outs than the current production version. Finally, we make some recommendations for tackling bias-variance tradeoffs in program analysis.},
	urldate = {2024-07-15},
	booktitle = {Proceedings of the 41st {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {Association for Computing Machinery},
	author = {Sharma, Rahul and Aiken, Alex},
	collaborator = {Nori, Aditya V.},
	year = {2014},
	pages = {127--137},
	file = {Full Text:C\:\\Users\\Admin\\Zotero\\storage\\MZ34423C\\Sharma et al. - 2014 - Bias-variance tradeoffs in program analysis.pdf:application/pdf},
}

@misc{noauthor_bias-variance_nodate,
	title = {Bias-variance tradeoffs in program analysis {\textbar} {Proceedings} of the 41st {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	url = {https://dl.acm.org/doi/10.1145/2535838.2535853},
	urldate = {2024-07-16},
	file = {Bias-variance tradeoffs in program analysis | Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages:C\:\\Users\\Admin\\Zotero\\storage\\YTJMXVAR\\2535838.html:text/html},
}

@misc{mehrabi_survey_2022,
	title = {A {Survey} on {Bias} and {Fairness} in {Machine} {Learning}},
	url = {http://arxiv.org/abs/1908.09635},
	doi = {10.48550/arXiv.1908.09635},
	abstract = {With the widespread use of AI systems and applications in our everyday lives, it is important to take fairness issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in machine learning, natural language processing, and deep learning that addresses such challenges in different subdomains. With the commercialization of these systems, researchers are becoming aware of the biases that these applications can contain and have attempted to address them. In this survey we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined in order to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and how they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
	urldate = {2024-07-16},
	publisher = {arXiv},
	author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
	month = jan,
	year = {2022},
	note = {arXiv:1908.09635 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\XE5WZBWY\\Mehrabi et al. - 2022 - A Survey on Bias and Fairness in Machine Learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\72UGJ4EC\\1908.html:text/html},
}

@misc{schaeffer_double_2023,
	title = {Double {Descent} {Demystified}: {Identifying}, {Interpreting} \& {Ablating} the {Sources} of a {Deep} {Learning} {Puzzle}},
	shorttitle = {Double {Descent} {Demystified}},
	url = {http://arxiv.org/abs/2303.14151},
	doi = {10.48550/arXiv.2303.14151},
	abstract = {Double descent is a surprising phenomenon in machine learning, in which as the number of model parameters grows relative to the number of data, test error drops as models grow ever larger into the highly overparameterized (data undersampled) regime. This drop in test error flies against classical learning theory on overfitting and has arguably underpinned the success of large models in machine learning. This non-monotonic behavior of test loss depends on the number of data, the dimensionality of the data and the number of model parameters. Here, we briefly describe double descent, then provide an explanation of why double descent occurs in an informal and approachable manner, requiring only familiarity with linear algebra and introductory probability. We provide visual intuition using polynomial regression, then mathematically analyze double descent with ordinary linear regression and identify three interpretable factors that, when simultaneously all present, together create double descent. We demonstrate that double descent occurs on real data when using ordinary linear regression, then demonstrate that double descent does not occur when any of the three factors are ablated. We use this understanding to shed light on recent observations in nonlinear models concerning superposition and double descent. Code is publicly available.},
	urldate = {2024-07-16},
	publisher = {arXiv},
	author = {Schaeffer, Rylan and Khona, Mikail and Robertson, Zachary and Boopathy, Akhilan and Pistunova, Kateryna and Rocks, Jason W. and Fiete, Ila Rani and Koyejo, Oluwasanmi},
	month = mar,
	year = {2023},
	note = {arXiv:2303.14151 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\UCKNCDW3\\Schaeffer et al. - 2023 - Double Descent Demystified Identifying, Interpret.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\6W2R97EC\\2303.html:text/html},
}

@misc{hellstrom_bias_2020,
	title = {Bias in {Machine} {Learning} -- {What} is it {Good} for?},
	url = {http://arxiv.org/abs/2004.00686},
	doi = {10.48550/arXiv.2004.00686},
	abstract = {In public media as well as in scientific publications, the term {\textbackslash}emph\{bias\} is used in conjunction with machine learning in many different contexts, and with many different meanings. This paper proposes a taxonomy of these different meanings, terminology, and definitions by surveying the, primarily scientific, literature on machine learning. In some cases, we suggest extensions and modifications to promote a clear terminology and completeness. The survey is followed by an analysis and discussion on how different types of biases are connected and depend on each other. We conclude that there is a complex relation between bias occurring in the machine learning pipeline that leads to a model, and the eventual bias of the model (which is typically related to social discrimination). The former bias may or may not influence the latter, in a sometimes bad, and sometime good way.},
	urldate = {2024-07-16},
	publisher = {arXiv},
	author = {Hellström, Thomas and Dignum, Virginia and Bensch, Suna},
	month = sep,
	year = {2020},
	note = {arXiv:2004.00686 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\KHQLFDXR\\Hellström et al. - 2020 - Bias in Machine Learning -- What is it Good for.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\DKPSRCAK\\2004.html:text/html},
}

@misc{nakkiran_deep_2019,
	title = {Deep {Double} {Descent}: {Where} {Bigger} {Models} and {More} {Data} {Hurt}},
	shorttitle = {Deep {Double} {Descent}},
	url = {http://arxiv.org/abs/1912.02292},
	doi = {10.48550/arXiv.1912.02292},
	abstract = {We show that a variety of modern deep learning tasks exhibit a "double-descent" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
	urldate = {2024-07-16},
	publisher = {arXiv},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	month = dec,
	year = {2019},
	note = {arXiv:1912.02292 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: G.K. and Y.B. contributed equally},
	file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\VVVJSJI4\\Nakkiran et al. - 2019 - Deep Double Descent Where Bigger Models and More .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\QH63P2SG\\1912.html:text/html},
}

@article{molavi_model_2024,
	title = {Model {Complexity}, {Expectations}, and {Asset} {Prices}},
	volume = {91},
	issn = {0034-6527},
	url = {https://doi.org/10.1093/restud/rdad073},
	doi = {10.1093/restud/rdad073},
	abstract = {This paper analyses how limits to the complexity of statistical models used by market participants can shape asset prices. We consider an economy in which the stochastic process that governs the evolution of economic variables may not have a simple representation, and yet, agents are only capable of entertaining statistical models with a certain level of complexity. As a result, they may end up with a lower-dimensional approximation that does not fully capture the intertemporal complexity of the true data-generating process. We first characterize the implications of the resulting departure from rational expectations and relate the extent of return and forecast-error predictability at various horizons to the complexity of agents’ models and the statistical properties of the underlying process. We then apply our framework to study violations of uncovered interest rate parity in foreign exchange markets. We find that constraints on the complexity of agents’ models can generate return predictability patterns that are simultaneously consistent with the well-known forward discount and predictability reversal puzzles.},
	number = {4},
	urldate = {2024-07-16},
	journal = {The Review of Economic Studies},
	author = {Molavi, Pooya and Tahbaz-Salehi, Alireza and Vedolin, Andrea},
	month = jul,
	year = {2024},
	pages = {2462--2507},
	file = {Full Text:C\:\\Users\\Admin\\Zotero\\storage\\I793BKYB\\Molavi et al. - 2024 - Model Complexity, Expectations, and Asset Prices.pdf:application/pdf;Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\GJFKDZMR\\7222145.html:text/html},
}

@misc{modelcomplex_exp,
	title = {Model {Complexity}, {Expectations}, and {Asset} {Prices} {\textbar} {The} {Review} of {Economic} {Studies} {\textbar} {Oxford} {Academic}},
    author={Molavi et al.},
	url = {https://academic.oup.com/restud/article-abstract/91/4/2462/7222145?redirectedFrom=fulltext&login=false},
	urldate = {2024-07-16},
	file = {Model Complexity, Expectations, and Asset Prices | The Review of Economic Studies | Oxford Academic:C\:\\Users\\Admin\\Zotero\\storage\\58K7TNWL\\7222145.html:text/html},
}

@misc{buschjager_generalized_2020,
	title = {Generalized {Negative} {Correlation} {Learning} for {Deep} {Ensembling}},
	url = {http://arxiv.org/abs/2011.02952},
	doi = {10.48550/arXiv.2011.02952},
	abstract = {Ensemble algorithms offer state of the art performance in many machine learning applications. A common explanation for their excellent performance is due to the bias-variance decomposition of the mean squared error which shows that the algorithm's error can be decomposed into its bias and variance. Both quantities are often opposed to each other and ensembles offer an effective way to manage them as they reduce the variance through a diverse set of base learners while keeping the bias low at the same time. Even though there have been numerous works on decomposing other loss functions, the exact mathematical connection is rarely exploited explicitly for ensembling, but merely used as a guiding principle. In this paper, we formulate a generalized bias-variance decomposition for arbitrary twice differentiable loss functions and study it in the context of Deep Learning. We use this decomposition to derive a Generalized Negative Correlation Learning (GNCL) algorithm which offers explicit control over the ensemble's diversity and smoothly interpolates between the two extremes of independent training and the joint training of the ensemble. We show how GNCL encapsulates many previous works and discuss under which circumstances training of an ensemble of Neural Networks might fail and what ensembling method should be favored depending on the choice of the individual networks. We make our code publicly available under https://github.com/sbuschjaeger/gncl},
	urldate = {2024-07-16},
	publisher = {arXiv},
	author = {Buschjäger, Sebastian and Pfahler, Lukas and Morik, Katharina},
	month = dec,
	year = {2020},
	note = {arXiv:2011.02952 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 12 (+8) pages, 1(+1) figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\VKP9LARU\\Buschjäger et al. - 2020 - Generalized Negative Correlation Learning for Deep.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\SUSWFMW5\\2011.html:text/html},
}

@misc{buschjager_generalized_2020-1,
	title = {Generalized {Negative} {Correlation} {Learning} for {Deep} {Ensembling}},
	url = {http://arxiv.org/abs/2011.02952},
	abstract = {Ensemble algorithms offer state of the art performance in many machine learning applications. A common explanation for their excellent performance is due to the bias-variance decomposition of the mean squared error which shows that the algorithm’s error can be decomposed into its bias and variance. Both quantities are often opposed to each other and ensembles offer an effective way to manage them as they reduce the variance through a diverse set of base learners while keeping the bias low at the same time. Even though there have been numerous works on decomposing other loss functions, the exact mathematical connection is rarely exploited explicitly for ensembling, but merely used as a guiding principle. In this paper, we formulate a generalized bias-variance decomposition for arbitrary twice differentiable loss functions and study it in the context of Deep Learning. We use this decomposition to derive a Generalized Negative Correlation Learning (GNCL) algorithm which offers explicit control over the ensemble’s diversity and smoothly interpolates between the two extremes of independent training and the joint training of the ensemble. We show how GNCL encapsulates many previous works and discuss under which circumstances training of an ensemble of Neural Networks might fail and what ensembling method should be favored depending on the choice of the individual networks. We make our code publicly available under https: //github.com/sbuschjaeger/gncl.},
	language = {en},
	urldate = {2024-07-16},
	publisher = {arXiv},
	author = {Buschjäger, Sebastian and Pfahler, Lukas and Morik, Katharina},
	month = dec,
	year = {2020},
	note = {arXiv:2011.02952 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 12 (+8) pages, 1(+1) figures},
	file = {Buschjäger et al. - 2020 - Generalized Negative Correlation Learning for Deep.pdf:C\:\\Users\\Admin\\Zotero\\storage\\2GUESS48\\Buschjäger et al. - 2020 - Generalized Negative Correlation Learning for Deep.pdf:application/pdf},
}

@article{belkin_reconciling_2019,
	title = {Reconciling modern machine learning practice and the bias-variance trade-off},
	volume = {116},
	issn = {0027-8424, 1091-6490},
	url = {http://arxiv.org/abs/1812.11118},
	doi = {10.1073/pnas.1903070116},
	abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias-variance trade-off, appears to be at odds with the observed behavior of methods used in the modern machine learning practice. The bias-variance trade-off implies that a model should balance under-fitting and over-fitting: rich enough to express underlying structure in data, simple enough to avoid fitting spurious patterns. However, in the modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered over-fit, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This "double descent" curve subsumes the textbook U-shaped bias-variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine learning models delineates the limits of classical analyses, and has implications for both the theory and practice of machine learning.},
	number = {32},
	urldate = {2024-07-16},
	journal = {Proc. Natl. Acad. Sci. U.S.A.},
	author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
	month = aug,
	year = {2019},
	note = {arXiv:1812.11118 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {15849--15854},
	file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\ZMYPPK75\\Belkin et al. - 2019 - Reconciling modern machine learning practice and t.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\258WG5FI\\1812.html:text/html},
}

@article{belkin_reconciling_2019-1,
	title = {Reconciling modern machine learning practice and the bias-variance trade-off},
	volume = {116},
	issn = {0027-8424, 1091-6490},
	url = {http://arxiv.org/abs/1812.11118},
	doi = {10.1073/pnas.1903070116},
	abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the ﬁeld, the bias-variance trade-oﬀ, appears to be at odds with the observed behavior of methods used in the modern machine learning practice. The bias-variance trade-oﬀ implies that a model should balance under-ﬁtting and over-ﬁtting: rich enough to express underlying structure in data, simple enough to avoid ﬁtting spurious patterns. However, in the modern practice, very rich models such as neural networks are trained to exactly ﬁt (i.e., interpolate) the data. Classically, such models would be considered over-ﬁt, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners.},
	language = {en},
	number = {32},
	urldate = {2024-07-16},
	journal = {Proc. Natl. Acad. Sci. U.S.A.},
	author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
	month = aug,
	year = {2019},
	note = {arXiv:1812.11118 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {15849--15854},
	file = {Belkin et al. - 2019 - Reconciling modern machine learning practice and t.pdf:C\:\\Users\\Admin\\Zotero\\storage\\SSIRTC5X\\Belkin et al. - 2019 - Reconciling modern machine learning practice and t.pdf:application/pdf},
}

@misc{yang_rethinking_2020,
	title = {Rethinking {Bias}-{Variance} {Trade}-off for {Generalization} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/2002.11328},
	abstract = {The classical bias-variance trade-off predicts that bias decreases and variance increases with model complexity, leading to a U-shaped risk curve. Recent work calls this into question for neural networks and other over-parameterized models, for which it is often observed that larger models generalize better. We provide a simple explanation for this by measuring the bias and variance of neural networks: while the bias is monotonically decreasing as in the classical theory, the variance is unimodal or bell-shaped: it increases then decreases with the width of the network. We vary the network architecture, loss function, and choice of dataset and conﬁrm that variance unimodality occurs robustly for all models we considered. The risk curve is the sum of the bias and variance curves and displays different qualitative shapes depending on the relative scale of bias and variance, with the double descent curve observed in recent literature as a special case. We corroborate these empirical results with a theoretical analysis of two-layer linear networks with random ﬁrst layer. Finally, evaluation on out-ofdistribution data shows that most of the drop in accuracy comes from increased bias while variance increases by a relatively small amount. Moreover, we ﬁnd that deeper models decrease bias and increase variance for both in-distribution and out-of-distribution data.},
	language = {en},
	urldate = {2024-07-16},
	publisher = {arXiv},
	author = {Yang, Zitong and Yu, Yaodong and You, Chong and Steinhardt, Jacob and Ma, Yi},
	month = dec,
	year = {2020},
	note = {arXiv:2002.11328 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Yang et al. - 2020 - Rethinking Bias-Variance Trade-off for Generalizat.pdf:C\:\\Users\\Admin\\Zotero\\storage\\MNRLKVWJ\\Yang et al. - 2020 - Rethinking Bias-Variance Trade-off for Generalizat.pdf:application/pdf},
}

@misc{unified_bias_composition,
	title = {[{PDF}] {A} {Unifeid} {Bias}-{Variance} {Decomposition} and its {Applications} {\textbar} {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/paper/A-Unifeid-Bias-Variance-Decomposition-and-its-Domingos/e1ed9d24db5e8f7ab326aeb797e965a94f5ad6d3},
	urldate = {2024-07-16},
	file = {[PDF] A Unifeid Bias-Variance Decomposition and its Applications | Semantic Scholar:C\:\\Users\\Admin\\Zotero\\storage\\BYNSTK6A\\e1ed9d24db5e8f7ab326aeb797e965a94f5ad6d3.html:text/html},
}

@misc{noauthor_pdf_nodate-1,
	title = {[{PDF}] {A} {Unifeid} {Bias}-{Variance} {Decomposition} and its {Applications} {\textbar} {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/paper/A-Unifeid-Bias-Variance-Decomposition-and-its-Domingos/e1ed9d24db5e8f7ab326aeb797e965a94f5ad6d3},
	urldate = {2024-07-16},
	file = {[PDF] A Unifeid Bias-Variance Decomposition and its Applications | Semantic Scholar:C\:\\Users\\Admin\\Zotero\\storage\\33UU52JB\\e1ed9d24db5e8f7ab326aeb797e965a94f5ad6d3.html:text/html},
}

@inproceedings{domingos_unifeid_2000,
	title = {A {Unifeid} {Bias}-{Variance} {Decomposition} and its {Applications}},
	url = {https://www.semanticscholar.org/paper/A-Unifeid-Bias-Variance-Decomposition-and-its-Domingos/e1ed9d24db5e8f7ab326aeb797e965a94f5ad6d3},
	abstract = {This paper presents a unified bias-variance decomposition that is applicable to squared loss, zero-one loss, variable misclassification costs, and other loss functions. The unified decomposition sheds light on a number of significant issues: the relation between some of the previously-proposed decompositions for zero-one loss and the original one for squared loss, the relation between bias, variance and Schapire et al.’s (1997) notion of margin, and the nature of the trade-off between bias and variance in classification. While the biasvariance behavior of zero-one loss and variable misclassification costs is quite different from that of squared loss, this difference derives directly from the different definitions of loss. We have applied the proposed decomposition to decision tree learning, instancebased learning and boosting on a large suite of benchmark data sets, and made several significant observations.},
	urldate = {2024-07-16},
	author = {Domingos, Pedro M.},
	month = jun,
	year = {2000},
	annote = {[TLDR] A unified bias-variance decomposition that is applicable to squared loss, zero-one loss, variable misclassification costs, and other loss functions is presented and applied to decision tree learning, instancebased learning and boosting on a large suite of benchmark data sets.},
	booktitle = {Semantic Scholar},
}

@misc{khan_bayesian_2024,
	title = {The {Bayesian} {Learning} {Rule}},
	url = {http://arxiv.org/abs/2107.04562},
	doi = {10.48550/arXiv.2107.04562},
	abstract = {We show that many machine-learning algorithms are specific instances of a single algorithm called the {\textbackslash}emph\{Bayesian learning rule\}. The rule, derived from Bayesian principles, yields a wide-range of algorithms from fields such as optimization, deep learning, and graphical models. This includes classical algorithms such as ridge regression, Newton's method, and Kalman filter, as well as modern deep-learning algorithms such as stochastic-gradient descent, RMSprop, and Dropout. The key idea in deriving such algorithms is to approximate the posterior using candidate distributions estimated by using natural gradients. Different candidate distributions result in different algorithms and further approximations to natural gradients give rise to variants of those algorithms. Our work not only unifies, generalizes, and improves existing algorithms, but also helps us design new ones.},
	urldate = {2024-07-16},
	publisher = {arXiv},
	author = {Khan, Mohammad Emtiyaz and Rue, Håvard},
	month = jun,
	year = {2024},
	note = {arXiv:2107.04562 [cs, stat]
version: 4},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\BZLE9337\\Khan and Rue - 2024 - The Bayesian Learning Rule.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\KTD5IIT7\\2107.html:text/html},
}



@misc{hu_model_2021,
	title = {Model {Complexity} of {Deep} {Learning}: {A} {Survey}},
	shorttitle = {Model {Complexity} of {Deep} {Learning}},
	url = {http://arxiv.org/abs/2103.05127},
	doi = {10.48550/arXiv.2103.05127},
	abstract = {Model complexity is a fundamental problem in deep learning. In this paper we conduct a systematic overview of the latest studies on model complexity in deep learning. Model complexity of deep learning can be categorized into expressive capacity and effective model complexity. We review the existing studies on those two categories along four important factors, including model framework, model size, optimization process and data complexity. We also discuss the applications of deep learning model complexity including understanding model generalization, model optimization, and model selection and design. We conclude by proposing several interesting future directions.},
	urldate = {2024-07-18},
	publisher = {arXiv},
	author = {Hu, Xia and Chu, Lingyang and Pei, Jian and Liu, Weiqing and Bian, Jiang},
	month = aug,
	year = {2021},
	note = {arXiv:2103.05127 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Admin\\Zotero\\storage\\QH5KVU4S\\Hu et al. - 2021 - Model Complexity of Deep Learning A Survey.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\9SIHWB6Z\\2103.html:text/html},
}

@misc{noauthor_neural_nodate,
	title = {Neural {Networks} and the {Bias}/{Variance} {Dilemma} {\textbar} {MIT} {Press} {Journals} \& {Magazine} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/document/6797087},
	urldate = {2024-07-18},
	file = {Neural Networks and the Bias/Variance Dilemma | MIT Press Journals & Magazine | IEEE Xplore:C\:\\Users\\Admin\\Zotero\\storage\\AU5FSEWY\\6797087.html:text/html},
}

@inproceedings{quetu_can_2023,
	title = {Can we avoid {Double} {Descent} in {Deep} {Neural} {Networks}?},
	url = {http://arxiv.org/abs/2302.13259},
	doi = {10.1109/ICIP49359.2023.10222624},
	abstract = {Finding the optimal size of deep learning models is very actual and of broad impact, especially in energy-saving schemes. Very recently, an unexpected phenomenon, the “double descent”, has caught the attention of the deep learning community. As the model’s size grows, the performance gets first worse and then goes back to improving. It raises serious questions about the optimal model’s size to maintain high generalization: the model needs to be sufficiently over-parametrized, but adding too many parameters wastes training resources. Is it possible to find, in an efficient way, the best trade-off? Our work shows that the double descent phenomenon is potentially avoidable with proper conditioning of the learning problem, but a final answer is yet to be found. We empirically observe that there is hope to dodge the double descent in complex scenarios with proper regularization, as a simple ℓ2 regularization is already positively contributing to such a perspective.},
	language = {en},
	urldate = {2024-08-26},
	booktitle = {2023 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Quétu, Victor and Tartaglione, Enzo},
	month = oct,
	year = {2023},
	note = {arXiv:2302.13259 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	pages = {1625--1629},
	file = {Quétu and Tartaglione - 2023 - Can we avoid Double Descent in Deep Neural Network.pdf:C\:\\Users\\Admin\\Zotero\\storage\\MW5WES7Q\\Quétu and Tartaglione - 2023 - Can we avoid Double Descent in Deep Neural Network.pdf:application/pdf},
}

@misc{lafon_understanding_2024,
	title = {Understanding the {Double} {Descent} {Phenomenon} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/2403.10459},
	abstract = {Combining empirical risk minimization with capacity control is a classical strategy in machine learning when trying to control the generalization gap and avoid overfitting, as the model class capacity gets larger. Yet, in modern deep learning practice, very large over-parameterized models (e.g. neural networks) are optimized to fit perfectly the training data and still obtain great generalization performance. Past the interpolation point, increasing model complexity seems to actually lower the test error.},
	language = {en},
	urldate = {2024-08-26},
	publisher = {arXiv},
	author = {Lafon, Marc and Thomas, Alexandre},
	month = mar,
	year = {2024},
	note = {arXiv:2403.10459 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Lafon and Thomas - 2024 - Understanding the Double Descent Phenomenon in Dee.pdf:C\:\\Users\\Admin\\Zotero\\storage\\VUY4KCYC\\Lafon and Thomas - 2024 - Understanding the Double Descent Phenomenon in Dee.pdf:application/pdf},
}

@misc{davies_unifying_2023,
	title = {Unifying {Grokking} and {Double} {Descent}},
	url = {http://arxiv.org/abs/2303.06173},
	abstract = {A principled understanding of generalization in deep learning may require unifying disparate observations under a single conceptual framework. Previous work has studied grokking, a training dynamic in which a sustained period of near-perfect training performance and near-chance test performance is eventually followed by generalization, as well as the superﬁcially similar double descent. These topics have so far been studied in isolation. We hypothesize that grokking and double descent can be understood as instances of the same learning dynamics within a framework of pattern learning speeds. We propose that this framework also applies when varying model capacity instead of optimization steps, and provide the ﬁrst demonstration of model-wise grokking.},
	language = {en},
	urldate = {2024-08-26},
	publisher = {arXiv},
	author = {Davies, Xander and Langosco, Lauro and Krueger, David},
	month = mar,
	year = {2023},
	note = {arXiv:2303.06173 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: ML Safety Workshop, 36th Conference on Neural Information Processing Systems (NeurIPS 2022)},
	file = {Davies et al. - 2023 - Unifying Grokking and Double Descent.pdf:C\:\\Users\\Admin\\Zotero\\storage\\7F3CMF7D\\Davies et al. - 2023 - Unifying Grokking and Double Descent.pdf:application/pdf},
}

@inproceedings{quetu_can_2023-1,
	title = {Can we avoid {Double} {Descent} in {Deep} {Neural} {Networks}?},
	url = {http://arxiv.org/abs/2302.13259},
	doi = {10.1109/ICIP49359.2023.10222624},
	abstract = {Finding the optimal size of deep learning models is very actual and of broad impact, especially in energy-saving schemes. Very recently, an unexpected phenomenon, the “double descent”, has caught the attention of the deep learning community. As the model’s size grows, the performance gets first worse and then goes back to improving. It raises serious questions about the optimal model’s size to maintain high generalization: the model needs to be sufficiently over-parametrized, but adding too many parameters wastes training resources. Is it possible to find, in an efficient way, the best trade-off? Our work shows that the double descent phenomenon is potentially avoidable with proper conditioning of the learning problem, but a final answer is yet to be found. We empirically observe that there is hope to dodge the double descent in complex scenarios with proper regularization, as a simple ℓ2 regularization is already positively contributing to such a perspective.},
	language = {en},
	urldate = {2024-08-26},
	booktitle = {2023 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Quétu, Victor and Tartaglione, Enzo},
	month = oct,
	year = {2023},
	note = {arXiv:2302.13259 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	pages = {1625--1629},
	file = {Quétu and Tartaglione - 2023 - Can we avoid Double Descent in Deep Neural Network.pdf:C\:\\Users\\Admin\\Zotero\\storage\\RKFF77M8\\Quétu and Tartaglione - 2023 - Can we avoid Double Descent in Deep Neural Network.pdf:application/pdf},
}

@inproceedings{d_ascoli_triple_2020,
	title = {Triple descent and the two kinds of overfitting: where \& why do they appear?},
	volume = {33},
	shorttitle = {Triple descent and the two kinds of overfitting},
	url = {https://proceedings.neurips.cc/paper/2020/hash/1fd09c5f59a8ff35d499c0ee25a1d47e-Abstract.html},
	abstract = {A recent line of research has highlighted the existence of a ``double descent'' phenomenon in deep learning, whereby increasing the number of training examples N causes the generalization error of neural networks to peak when N is of the same order as the number of parameters P. In earlier works, a similar phenomenon was shown to exist in simpler models such as linear regression, where the peak instead occurs when N is equal to the input dimension D. Since both peaks coincide with the interpolation threshold, they are often conflated in the litterature. In this paper, we show that despite their apparent similarity, these two scenarios are inherently different. In fact, both peaks can co-exist when neural networks are applied to noisy regression tasks. The relative size of the peaks is then governed by the degree of nonlinearity of the activation function. Building on recent developments in the analysis of random feature models, we provide a theoretical ground for this sample-wise triple descent. As shown previously, the nonlinear peak at N=P is a true divergence caused by the extreme sensitivity of the output function to both the noise corrupting the labels and the initialization of the random features (or the weights in neural networks). This peak survives in the absence of noise, but can be suppressed by regularization. In contrast, the linear peak at N=D is solely due to overfitting the noise in the labels, and forms earlier during training. We show that this peak is implicitly regularized by the nonlinearity, which is why it only becomes salient at high noise and is weakly affected by explicit regularization.
Throughout the paper, we compare the analytical results obtained in the random feature model with the outcomes of numerical experiments involving realistic neural networks.},
	urldate = {2024-08-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {d' Ascoli, Stéphane and Sagun, Levent and Biroli, Giulio},
	year = {2020},
	pages = {3058--3069},
	file = {Full Text PDF:C\:\\Users\\Admin\\Zotero\\storage\\BMDU7UMD\\d' Ascoli et al. - 2020 - Triple descent and the two kinds of overfitting w.pdf:application/pdf},
}
@misc{liu2023understandingroleoptimizationdouble,
      title={Understanding the Role of Optimization in Double Descent}, 
      author={Chris Yuhao Liu and Jeffrey Flanigan},
      year={2023},
      eprint={2312.03951},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.03951}, 
}

@misc{olmin2024understandingepochwisedoubledescent,
      title={Towards Understanding Epoch-wise Double descent in Two-layer Linear Neural Networks}, 
      author={Amanda Olmin and Fredrik Lindsten},
      year={2024},
      eprint={2407.09845},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2407.09845}, 
}
@inproceedings{Domingos2000AUB,
  title={A Unified Bias-Variance Decomposition for Zero-One and Squared Loss},
  author={Pedro M. Domingos},
  booktitle={AAAI/IAAI},
  year={2000},
  url={https://api.semanticscholar.org/CorpusID:2063488}
}

@article{GRP_Hamilton,
author={Hamilton, William L.},
title={Graph Representation Learning},
journal={Synthesis Lectures on Artificial Intelligence and Machine Learning},
volume={14},
number={3},
pages={1-159},
publisher={Morgan and Claypool}
}

@book{10.5555/2371238,
author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
title = {Foundations of Machine Learning},
year = {2012},
isbn = {026201825X},
publisher = {The MIT Press},
abstract = {This graduate-level textbook introduces fundamental concepts and methods in machine learning. It describes several important modern algorithms, provides the theoretical underpinnings of these algorithms, and illustrates key aspects for their application. The authors aim to present novel theoretical tools and concepts while giving concise proofs even for relatively advanced topics. Foundations of Machine Learning fills the need for a general textbook that also offers theoretical details and an emphasis on proofs. Certain topics that are often treated with insufficient attention are discussed in more detail here; for example, entire chapters are devoted to regression, multi-class classification, and ranking. The first three chapters lay the theoretical foundation for what follows, but each remaining chapter is mostly self-contained. The appendix offers a concise probability review, a short introduction to convex optimization, tools for concentration bounds, and several basic properties of matrices and norms used in the book. The book is intended for graduate students and researchers in machine learning, statistics, and related areas; it can be used either as a textbook or as a reference text for a research seminar.}
}

@book{10.5555/2930837,
author = {Sugiyama, Masashi},
title = {Introduction to Statistical Machine Learning},
year = {2015},
isbn = {9780128023501},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Machine learning allows computers to learn and discern patterns without actually being programmed. When Statistical techniques and machine learning are combined together they are a powerful tool for analysing various kinds of data in many computer science/engineering areas including, image processing, speech processing, natural language processing, robot control, as well as in fundamental sciences such as biology, medicine, astronomy, physics, and materials. Introduction to Statistical Machine Learning provides a general introduction to machine learning that covers a wide range of topics concisely and will help you bridge the gap between theory and practice. Part I discusses the fundamental concepts of statistics and probability that are used in describing machine learning algorithms. Part II and Part III explain the two major approaches of machine learning techniques; generative methods and discriminative methods. While Part III provides an in-depth look at advanced topics that play essential roles in making machine learning algorithms more useful in practice. The accompanying MATLAB/Octave programs provide you with the necessary practical skills needed to accomplish a wide range of data analysis tasks. Provides the necessary background material to understand machine learning such as statistics, probability, linear algebra, and calculus. Complete coverage of the generative approach to statistical pattern recognition and the discriminative approach to statistical machine learning. Includes MATLAB/Octave programs so that readers can test the algorithms numerically and acquire both mathematical and practical skills in a wide range of data analysis tasks Discusses a wide range of applications in machine learning and statistics and provides examples drawn from image processing, speech processing, natural language processing, robot control, as well as biology, medicine, astronomy, physics, and materials. Table of Contents Part I: Introduction to Statistics and Probability 1. Random variables and probability distributions 2. Examples of discrete probability distributions 3. Examples of continuous probability distributions 4. Multi-dimensional probability distributions 5. Examples of multi-dimensional probability distributions 6. Random sample generation from arbitrary probability distributions 7. Probability distributions of the sum of independent random variables 8. Probability inequalities 9. Statistical inference 10. Hypothesis testing Part II: Generative Approach to Statistical Pattern Recognition 11. Fundamentals of statistical pattern recognition 12. Criteria for developing classifiers 13. Maximum likelihood estimation 14. Theoretical properties of maximum likelihood estimation 15. Linear discriminant analysis 16. Model selection for maximum likelihood estimation 17. Maximum likelihood estimation for Gaussian mixture model 18. Bayesian inference 19. Numerical computation in Bayesian inference 20. Model selection in Bayesian inference 21. Kernel density estimation 22. Nearest neighbor density estimation Part III: Discriminative Approach to Statistical Machine Learning 23. Fundamentals of statistical machine learning 24. Learning Models 25. Least-squares regression 26. Constrained least-squares regression 27. Sparse regression 28. Robust regression 29. Least-squares classification 30. Support vector classification 31. Ensemble classification 32. Probabilistic classification 33. Structured classification Part IV: Further Topics 34. Outlier detection 35. Unsupervised dimensionality reduction 36. Clustering 37. Online learning 38. Semi-supervised learning 39. Supervised dimensionality reduction 40. Transfer learning 41. Multi-task learning}
}
@book{Vapnik1999-VAPTNO,
	author = {Vladimir Vapnik},
	editor = {},
	publisher = {Springer: New York},
	title = {The Nature of Statistical Learning Theory},
	year = {1999}
}

@ARTICLE{Scar04,
  author={Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  journal={IEEE Transactions on Neural Networks}, 
  title={The Graph Neural Network Model}, 
  year={2009},
  volume={20},
  number={1},
  pages={61-80},
  keywords={Neural networks;Biological system modeling;Data engineering;Computer vision;Chemistry;Biology;Pattern recognition;Data mining;Supervised learning;Parameter estimation;Graphical domains;graph neural networks (GNNs);graph processing;recursive neural networks},
  doi={10.1109/TNN.2008.2005605}}

@article{Bronstein_2017,
   title={Geometric Deep Learning: Going beyond Euclidean data},
   volume={34},
   ISSN={1558-0792},
   url={http://dx.doi.org/10.1109/MSP.2017.2693418},
   DOI={10.1109/msp.2017.2693418},
   number={4},
   journal={IEEE Signal Processing Magazine},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Bronstein, Michael M. and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
   year={2017},
   month=jul, pages={18–42} }
@misc{bronstein2021geometricdeeplearninggrids,
      title={Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges}, 
      author={Michael M. Bronstein and Joan Bruna and Taco Cohen and Petar Veličković},
      year={2021},
      eprint={2104.13478},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2104.13478}, 
}

@article{Veli_kovi__2023,
   title={Everything is connected: Graph neural networks},
   volume={79},
   ISSN={0959-440X},
   url={http://dx.doi.org/10.1016/j.sbi.2023.102538},
   DOI={10.1016/j.sbi.2023.102538},
   journal={Current Opinion in Structural Biology},
   publisher={Elsevier BV},
   author={Veličković, Petar},
   year={2023},
   month=apr, pages={102538} }

@misc{tanis2024introductiongraphneuralnetworks,
      title={Introduction to Graph Neural Networks: A Starting Point for Machine Learning Engineers}, 
      author={James H. Tanis and Chris Giannella and Adrian V. Mariano},
      year={2024},
      eprint={2412.19419},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.19419}, 
}
@misc{lopushanskyy2024graphneuralnetworksgraph,
      title={Graph Neural Networks on Graph Databases}, 
      author={Dmytro Lopushanskyy and Borun Shi},
      year={2024},
      eprint={2411.11375},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.11375}, 
}
@ARTICLE{6797087,
  author={Geman, Stuart and Bienenstock, Elie and Doursat, René},
  journal={Neural Computation}, 
  title={Neural Networks and the Bias/Variance Dilemma}, 
  year={1992},
  volume={4},
  number={1},
  pages={1-58},
  keywords={},
  doi={10.1162/neco.1992.4.1.1}}


@misc{Scott_Fortmann_Bias,
	title = {Understanding the {Bias}-{Variance} {Tradeoff}},
	url = {https://scott.fortmann-roe.com/docs/BiasVariance.html},
	urldate = {2025-02-26},
	file = {Understanding the Bias-Variance Tradeoff:C\:\\Users\\Admin\\Zotero\\storage\\VTD3RSFB\\BiasVariance.html:text/html},
	author = {Scott Fortmann},
	year = {2012},
}

@article{mcculloch_logical_1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	issn = {1522-9602},
	url = {https://doi.org/10.1007/BF02478259},
	doi = {10.1007/BF02478259},
	abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	number = {4},
	journal = {The bulletin of mathematical biophysics},
	author = {McCulloch, Warren S. and Pitts, Walter},
	month = dec,
	year = {1943},
	pages = {115--133},
}

@misc{VeltenetalMathematicalModelling,
	year = 2024,
	title = {Mathematical {Modeling} and {Simulation}: {Introduction} for {Scientists} and {Engineers}, 2nd {Edition} {\textbar} {Wiley}},
	shorttitle = {Mathematical {Modeling} and {Simulation}},
	url = {https://www.wiley.com/en-us/Mathematical+Modeling+and+Simulation%3A+Introduction+for+Scientists+and+Engineers%2C+2nd+Edition-p-9783527839407},
	abstract = {{\textless}p{\textgreater}\textbf{Learn to use modeling and simulation methods to attack real-world problems, from physics to engineering, from life sciences to process engineering}{\textless}/p{\textgreater} {\textless}p{\textgreater}\textbf{Reviews of the \textit{first edition} (2009):}{\textless}/p{\textgreater} {\textless}p{\textgreater}"Perfectly fits introductory modeling courses [...] and is an enjoyable reading in the first place. Highly recommended [...]"{\textless}br /{\textgreater}\textbf{\textit{Zentralblatt MATH,} European Mathematical Society, 2009}{\textless}/p{\textgreater} {\textless}p{\textgreater}"This book differs from almost all other available modeling books in that [the authors address] both mechanistic and statistical models as well as 'hybrid' models. [...] The modeling range is enormous."{\textless}br /{\textgreater}\textbf{\textit{SIAM Society of Industrial and Applied Mathematics,} USA, 2011}{\textless}/p{\textgreater} {\textless}p{\textgreater}This completely revised and substantially extended second edition answers the most important questions in the field of modeling: What is a mathematical model? What types of models do exist? Which model is appropriate for a particular problem? What are simulation, parameter estimation, and validation? What kind of mathematical problems appear and how can these be efficiently solved using professional free of charge open source software?{\textless}/p{\textgreater} {\textless}p{\textgreater}The book addresses undergraduates and practitioners alike. Although only basic knowledge of calculus and linear algebra is required, the most important mathematical structures are discussed in sufficient detail, ranging from statistical models to partial differential equations and accompanied by examples from biology, ecology, economics, medicine, agricultural, chemical, electrical, mechanical, and process engineering.{\textless}/p{\textgreater} {\textless}p{\textgreater}About 200 pages of additional material include a unique chapter on virtualization, Crash Courses on the data analysis and programming languages R and Python and on the computer algebra language Maxima, many new methods and examples scattered throughout the book, an update of all software-related procedures, and a comprehensive book software providing templates for typical modeling tasks in thousands of code lines. The book software includes GmLinux, an operating system specifically designed for this book providing preconfigured and ready-to-use installations of OpenFOAM, Salome, FreeCAD/CfdOF workbench, ParaView, R, Maxima/wxMaxima, Python, Rstudio, Quarto/Markdown and other free of charge open source software used in the book.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2025-04-04},
	journal = {Wiley.com},
	file = {Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\GX5VRFMD\\Mathematical+Modeling+and+Simulation+Introduction+for+Scientists+and+Engineers,+2nd+Edition-p-9.html:text/html},
}
@misc{AIWashington,
  author = {Chris Smith et al.},
  howpublished = {Technical review},
  title = {The History of Artificial Intelligence},
  year = {2006}
}
@book{10.5555/1671238,
author = {Russell, Stuart and Norvig, Peter},
title = {Artificial Intelligence: A Modern Approach},
year = {2009},
isbn = {0136042597},
publisher = {Prentice Hall Press},
address = {USA},
edition = {3rd},
}
@InCollection{sep-frame-problem,
	author       =	{Shanahan, Murray},
	title        =	{{The Frame Problem}},
	booktitle    =	{The {Stanford} Encyclopedia of Philosophy},
	editor       =	{Edward N. Zalta},
	howpublished =	{\url{https://plato.stanford.edu/archives/spr2016/entries/frame-problem/}},
	year         =	{2016},
	edition      =	{{S}pring 2016},
	publisher    =	{Metaphysics Research Lab, Stanford University}
}
@article{FrameGryzJarek,
author = {Gryz, Jarek},
year = {2013},
month = {06},
pages = {15-30},
title = {The Frame Problem in Artificial Intelligence and Philosophy},
volume = {21},
journal = {Filozofia Nauki}
}
@article{SeagerFrameAxiological,
  author = {William Seager},
  journal = {Philosophical report},
  number = {},
  title = {Frame Problems, Emotions and Axiological Projectionism},
  volume = {},
  year = {2010s (or older)}
}
@inproceedings{Briggs2014MachineE,
  title={Machine Ethics , the Frame Problem , and Theory of Mind},
  author={Gordon Briggs},
  year={2014},
  url={https://api.semanticscholar.org/CorpusID:14954096}
}
@article{DreyfusImpasse1979,
  author = {Hubert L. Dreyfus},
  journal = {},
  title = {From Micro-Worlds to Knowledge Representation : AI at an Impasse},
  volume = {},
  year = {1979}
}
@book{Descartes1950-DESDOM,
	address = {Harmondsworth,},
	author = {Rene? Descartes},
	editor = {},
	publisher = {Harmondsworth, Penguin},
	title = {Discourse on Method},
	year = {1950}
}
@ARTICLE{1056797NewellSimon,
  author={Newell, A. and Simon, H.},
  journal={IRE Transactions on Information Theory}, 
  title={The logic theory machine--A complex information processing system}, 
  year={1956},
  volume={2},
  number={3},
  pages={61-79},
  keywords={Logic;Information processing;Problem-solving;Humans;Formal languages;Information analysis;Heuristic algorithms;Automatic programming;Pattern recognition},
  doi={10.1109/TIT.1956.1056797}}
@article{Goel_2022, 
	title={Looking Back, Looking Ahead: Symbolic versus Connectionist AI}, 
	volume={42}, 
	url={https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15111}, 
	DOI={10.1609/aaai.12026}, 
	abstractNote={&lt;p&gt;The ongoing debate between symbolic and connectionist AI attends to some of the most fundamental issues in the field. In this column, I briefly review the evolution of the unfolding discussion. I also point out that there is a lot more to intelligence than the symbolic and connectionist views of AI.&lt;/p&gt;}, number={4}, 
	journal={AI Magazine}, 
	author={Goel, Ashok}, 
	year={2022}, 
	month={Jan.}, 
	pages={83-85} }
@misc{abhishek2019introductionconcentrationinequalities,
      title={Introduction to Concentration Inequalities}, 
      author={Kumar Abhishek and Sneha Maheshwari and Sujit Gujar},
      year={2019},
      eprint={1910.02884},
      archivePrefix={arXiv},
      primaryClass={math.PR},
      url={https://arxiv.org/abs/1910.02884}, 
}

@book{boucheron_concentration_2013,
	title = {Concentration {Inequalities}: {A} {Nonasymptotic} {Theory} of {Independence}},
	isbn = {978-0-19-953525-5},
	url = {https://doi.org/10.1093/acprof:oso/9780199535255.001.0001},
	abstract = {This monograph presents a mathematical theory of concentration inequalities for functions of independent random variables. The basic phenomenon under investigation is that if a function of many independent random variables does not depend too much on any of them then it is concentrated around its expected value. This book offers a host of inequalities to quantify this statement. The authors describe the interplay between the probabilistic structure (independence) and a variety of tools ranging from functional inequalities, transportation arguments, to information theory. Applications to the study of empirical processes, random projections, random matrix theory, and threshold phenomena are presented. The book offers a self-contained introduction to concentration inequalities, including a survey of concentration of sums of independent random variables, variance bounds, the entropy method, and the transportation method. Deep connections with isoperimetric problems are revealed. Special attention is paid to applications to the supremum of empirical processes.},
	publisher = {Oxford University Press},
	author = {Boucheron, Stéphane and Lugosi, Gábor and Massart, Pascal},
	month = feb,
	year = {2013},
	doi = {10.1093/acprof:oso/9780199535255.001.0001},
	doi = {10.1093/acprof:oso/9780199535255.001.0001},
}
@misc{neal2019biasvariancetradeofftextbooksneed,
      title={On the Bias-Variance Tradeoff: Textbooks Need an Update}, 
      author={Brady Neal},
      year={2019},
      eprint={1912.08286},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1912.08286}, 
}
@article{Grenander1952OnES,
  title={On empirical spectral analysis of stochastic processes},
  author={Ulf Grenander},
  journal={Arkiv f{\"o}r Matematik},
  year={1952},
  volume={1},
  pages={503-531},
  url={https://api.semanticscholar.org/CorpusID:122878699}
}
@article{Sterkenburg_2024,
   title={Statistical Learning Theory and Occam’s Razor: The Core Argument},
   volume={35},
   ISSN={1572-8641},
   url={http://dx.doi.org/10.1007/s11023-024-09703-y},
   DOI={10.1007/s11023-024-09703-y},
   number={1},
   journal={Minds and Machines},
   publisher={Springer Science and Business Media LLC},
   author={Sterkenburg, Tom F.},
   year={2024},
   month=nov }
@book{gareth_james_introduction_2013,
	title = {An introduction to statistical learning : with applications in {R}},
	url = {https://search.library.wisc.edu/catalog/9910207152902121},
	abstract = {xiv, 426 pages : illustrations (some color) ; 24 cm},
	publisher = {New York : Springer, [2013] ©2013},
	author = {Gareth James and Trevor Hastie and Robert Tibshirani and Daniela Witten},
	year = {2013},
	annote = {Includes bibliographical references and index.},
}
@article{articleMetaxiotis,
author = {Metaxiotis, Kostas and Samouilidis, J-E},
year = {2000},
month = {05},
pages = {75-79},
title = {Expert Systems in Medicine: Academic Illusion or Real Power?},
volume = {8},
journal = {Information Management \& Computer Security},
doi = {10.1108/09685220010694017}
}
@book{10.5555/2721661,
author = {Demuth, Howard B. and Beale, Mark H. and De Jess, Orlando and Hagan, Martin T.},
title = {Neural Network Design},
year = {2014},
isbn = {0971732116},
publisher = {Martin Hagan},
address = {Stillwater, OK, USA},
edition = {2nd},
abstract = {This book, by the authors of the Neural Network Toolbox for MATLAB, provides a clear and detailed coverage of fundamental neural network architectures and learning rules. In it, the authors emphasize a coherent presentation of the principal neural networks, methods for training them and their applications to practical problems. Features Extensive coverage of training methods for both feedforward networks (including multilayer and radial basis networks) and recurrent networks. In addition to conjugate gradient and Levenberg-Marquardt variations of the backpropagation algorithm, the text also covers Bayesian regularization and early stopping, which ensure the generalization ability of trained networks. Associative and competitive networks, including feature maps and learning vector quantization, are explained with simple building blocks. A chapter of practical training tips for function approximation, pattern recognition, clustering and prediction, along with five chapters presenting detailed real-world case studies. Detailed examples and numerous solved problems. Slides and comprehensive demonstration software can be downloaded from hagan.okstate.edu/nnd.html.}
}
@misc{zhang2023divedeeplearning,
      title={Dive into Deep Learning}, 
      author={Aston Zhang and Zachary C. Lipton and Mu Li and Alexander J. Smola},
      year={2023},
      eprint={2106.11342},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2106.11342}, 
}
@article{WERKER198449,
title = {Cross-language speech perception: Evidence for perceptual reorganization during the first year of life},
journal = {Infant Behavior and Development},
volume = {7},
number = {1},
pages = {49-63},
year = {1984},
issn = {0163-6383},
doi = {https://doi.org/10.1016/S0163-6383(84)80022-3},
url = {https://www.sciencedirect.com/science/article/pii/S0163638384800223},
author = {Janet F. Werker and Richard C. Tees},
keywords = {infants, speech perception, cross-language, decline},
abstract = {Previous work in which we compared English infants, English adults, and Hindi adults on their ability to discriminate two pairs of Hindi (non-English) speech contrasts has indicated that infants discriminate speech sounds according to phonetic category without prior specific language experience (Werker, Gilbert, Humphrey, & Tees, 1981), whereas adults and children as young as age 4 (Werker & Tees, in press), may lose this ability as a function of age and or linguistic experience. The present work was designed to (a) determine the generalizability of such a decline by comparing adult English, adult Salish, and English infant subjects on their perception of a new non-English (Salish) speech contrast, and (b) delineate the time course of the developmental decline in this ability. The results of these experiments replicate our original findings by showing that infants can discriminate nonnative speech contrasts without relevant experience, and that there is a decline in this ability during ontogeny. Furthermore, data from both cross-sectional and longitudinal studies shows that this decline occurs within the first year of life, and that it is a function of specific language experience.}
}
@inbook{1180370208,
author={Eric R. Kandel and John D. Koester and Sarah H. Mack and Steven A. Siegelbaum},
title={},
booktitle={Principles of Neural Science, 6e},
publisher={McGraw Hill},
address={New York, NY},
year={2021},
url={accessbiomedicalscience.mhmedical.com/content.aspx?aid=1180370208}
}

@book{purves_neuroscience_2004,
	address = {Sunderland, MA, US},
	series = {Neuroscience, 3rd ed},
	title = {Neuroscience, 3rd ed},
	isbn = {978-0-87893-725-7},
	abstract = {Whether judged in molecular, cellular, systemic, behavioral, or cognitive terms, the human nervous system is a stupendous piece of biological machinery. Given its accomplishments--all the artifacts of human culture, for instance--there is good reason for wanting to understand how the brain and the rest of the nervous system works. The debilitating and costly effects of neurological and psychiatric disease add a further sense of urgency to this quest. The aim of this book is to highlight the intellectual challenges and excitement--as well as the uncertainties--of what many see as the last great frontier of biological science. The information presented should serve as a starting point for undergraduates, medical students, graduate students in the neurosciences, and others who want to understand how the human nervous system operates. Like any other great challenge, neuroscience should be, and is, full of debate, dissension, and considerable fun. All these ingredients have gone into the construction of the third edition of this book; we hope they will be conveyed in equal measure to readers at all levels. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	publisher = {Sinauer Associates},
	editor = {Purves, Dale and Augustine, George J. and Fitzpatrick, David and Hall, William C. and LaMantia, Anthony-Samuel and McNamara, James O. and Williams, S. Mark},
	year = {2004},
	note = {Pages: xix, 773},
	keywords = {Nervous System, Neurosciences},
	file = {Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\BVDKGHRS\\2004-16062-000.html:text/html},
}

@misc{neurondoctrinemishqat,
	title = {The {Neuron} {Doctrine} (1860-1895) {\textbar} {Embryo} {Project} {Encyclopedia}},
	author = {Isra Mishqat},
	url = {https://embryo.asu.edu/pages/neuron-doctrine-1860-1895},
	urldate = {2025-04-30},
	file = {The Neuron Doctrine (1860-1895) | Embryo Project Encyclopedia:C\:\\Users\\Admin\\Zotero\\storage\\27XY45F7\\neuron-doctrine-1860-1895.html:text/html},
}

@article{rozo_cajal_2024,
	title = {Cajal, the neuronal theory and the idea of brain plasticity},
	volume = {18},
	issn = {1662-5129},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10910026/},
	doi = {10.3389/fnana.2024.1331666},
	abstract = {This paper reviews the importance of Cajal’s neuronal theory (the Neuron Doctrine) and the origin and importance of the idea of brain plasticity that emerges from this theory. We first comment on the main Cajal’s discoveries that gave rise and confirmed his Neuron Doctrine: the improvement of staining techniques, his approach to morphological laws, the concepts of dynamic polarisation, neurogenesis and neurotrophic theory, his first discoveries of the nerve cell as an independent cell, his research on degeneration and regeneration and his fight against reticularism. Second, we review Cajal’s ideas on brain plasticity and the years in which they were published, to finally focus on the debate on the origin of the term plasticity and its conceptual meaning, and the originality of Cajal’s proposal compared to those of other authors of the time.},
	urldate = {2025-04-30},
	journal = {Front Neuroanat},
	author = {Rozo, Jairo A. and Martínez-Gallego, Irene and Rodríguez-Moreno, Antonio},
	month = feb,
	year = {2024},
	pmid = {38440067},
	pmcid = {PMC10910026},
	pages = {1331666},
	file = {Full Text:C\:\\Users\\Admin\\Zotero\\storage\\7GNHKKL2\\Rozo et al. - 2024 - Cajal, the neuronal theory and the idea of brain p.pdf:application/pdf},
}
@article{gerlach1872struktur,
  author       = {Gerlach, Joseph von},
  title        = {Ueber die Structur der grauen Substanz des menschlichen Grosshirns. Vorläufige Mitheilung},
  title_english= {On the Structure of the Grey Matter in the Human Cerebrum. Preliminary Communication},
  journal      = {Centralblatt für die medizinischen Wissenschaften},
  volume       = {10},
  number       = {},
  pages        = {273--288},
  year         = {1872},
  language     = {German},
  note         = {Introduced the reticular theory of the nervous system},
  url          = {https://www.booklooker.de/B%C3%BCcher/Joseph-Gerlach%2BUeber-die-Structur-der-grauen-Substanz-des-menschlichen-Grosshirns-Vorl%C3%A4ufige/id/A02ohXHY01ZZu},
}
@misc{liu2025kankolmogorovarnoldnetworks,
      title={KAN: Kolmogorov-Arnold Networks}, 
      author={Ziming Liu and Yixuan Wang and Sachin Vaidya and Fabian Ruehle and James Halverson and Marin Soljačić and Thomas Y. Hou and Max Tegmark},
      year={2025},
      eprint={2404.19756},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.19756}, 
}
@incollection{ANDRILLI20101,
title = {Chapter 1 - Vectors and Matrices},
editor = {Stephen Andrilli and David Hecker},
booktitle = {Elementary Linear Algebra (Fourth Edition)},
publisher = {Academic Press},
edition = {Fourth Edition},
address = {Boston},
pages = {1-77},
year = {2010},
isbn = {978-0-12-374751-8},
doi = {https://doi.org/10.1016/B978-0-12-374751-8.00001-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780123747518000019},
author = {Stephen Andrilli and David Hecker},
abstract = {Publisher Summary
This chapter defines vectors and describes their algebraic and geometric properties. It also introduces another fundamental object—matrix—whose basic properties parallel those of the vector and discusses the examination of techniques that are useful for reading and writing proofs. The link between algebraic manipulation and geometric intuition is a recurring theme in linear algebra that is used to establish many important results. The study of linear algebra begins with vectors and matrices—two of the most practical concepts in mathematics. Linear algebra, in addition to having a multitude of practical applications in science and engineering, also can be used to introduce proof-writing skills. The concept of proof is central to higher mathematics. Mathematicians claim no statement as a “fact” until it is proven true using logical deduction. Therefore, no one can succeed in higher mathematics without mastering the techniques required to supply such a proof.}
}
@book{10.5555/50066,
author = {Minsky, Marvin L. and Papert, Seymour A.},
title = {Perceptrons: expanded edition},
year = {1988},
isbn = {0262631113},
publisher = {MIT Press},
address = {Cambridge, MA, USA}
}
@article{JMLR:v11:shalev-shwartz10a,
  author  = {Shai Shalev-Shwartz and Ohad Shamir and Nathan Srebro and Karthik Sridharan},
  title   = {Learnability, Stability and Uniform Convergence},
  journal = {Journal of Machine Learning Research},
  year    = {2010},
  volume  = {11},
  number  = {90},
  pages   = {2635--2670},
  url     = {http://jmlr.org/papers/v11/shalev-shwartz10a.html}
}
@article{OstaszewskaZajkowski2014,
  author       = {Urszula Ostaszewska and Krzysztof Zajkowski},
  title        = {Cramér transform and t-entropy},
  journal      = {Positivity},
  volume       = {18},
  pages        = {347--358},
  year         = {2014},
  doi          = {10.1007/s11117-013-0247-3},
  url          = {https://doi.org/10.1007/s11117-013-0247-3},
}
@misc{liam_statistics_2005,
	author =  { Liam Paninski},
	year = {2005},
	title = {Statistics 4107: {Intro} to {Math} {Stat} (Fall 2005)},
	url = {https://sites.stat.columbia.edu/liam/teaching/4107-fall05/},
	urldate = {2025-05-25},
}
% === 1. PAC Learning & VC Dimension ===
@article{VapnikChervonenkis:1971,
  author = {Vapnik, Vladimir N. and Chervonenkis, Alexey Y.},
  title = {On the Uniform Convergence of Relative Frequencies to Their Probabilities},
  journal = {Theory of Probability and Its Applications},
  year = {1971},
  volume = {16},
  number = {2},
  pages = {264--280},
  note = {Classic VC uniform convergence result}
}
% === 2. Rademacher Complexity ===
@article{BartlettMendelson:2002:Rademacher,
  author = {Bartlett, Peter L. and Mendelson, Shahar},
  title = {Rademacher and Gaussian Complexities: Risk Bounds and Structural Results},
  journal = {Journal of Machine Learning Research},
  year = {2002},
  volume = {3},
  pages = {463--482},
  note = {Data-dependent complexity measures}
}

@article{BartlettBousquetMendelson:2005:LocalRademacher,
  author = {Bartlett, Peter L. and Bousquet, Olivier and Mendelson, Shahar},
  title = {Local Rademacher Complexities},
  journal = {Annals of Statistics},
  year = {2005},
  volume = {33},
  number = {4},
  pages = {1497--1537},
  note = {Sharper, localized capacity bounds}
}

% === 3. Stability-Based Generalization ===
@article{BousquetElisseeff:2002:Stability,
  author = {Bousquet, Olivier and Elisseeff, Andr\'e},
  title = {Stability and Generalization},
  journal = {Journal of Machine Learning Research},
  year = {2002},
  volume = {2},
  pages = {499--526},
  note = {Algorithmic stability bounds}
}

% === 4. PAC-Bayes Bounds ===
@inproceedings{McAllester:1999:PACBayes,
  author = {McAllester, David A.},
  title = {PAC-Bayesian Model Averaging},
  booktitle = {Proceedings of the 12th Annual Conference on Computational Learning Theory (COLT)},
  year = {1999},
  pages = {164--170},
  publisher = {ACM},
  note = {Foundational PAC-Bayes framework}
}

% === 5. Compression-Based Bounds ===
@article{FloydWarmuth:1995:SampleCompression,
  author = {Floyd, Stephen and Warmuth, Manfred K.},
  title = {Sample Compression, Learnability, and the Vapnik-Chervonenkis Dimension},
  journal = {Machine Learning},
  year = {1995},
  volume = {21},
  number = {3},
  pages = {269--304},
  note = {Compression bounds linking model size and generalization}
}

% === 6. Information-Theoretic Bounds ===
@inproceedings{RussoZou:2016:InformationTheory,
  author = {Russo, Daniel and Zou, James},
  title = {Controlling Bias in Adaptive Data Analysis Using Information Theory},
  booktitle = {Proceedings of AISTATS},
  year = {2016},
  note = {Introduced info-theoretic generalization bounds}
}

@inproceedings{XuRaginsky:2017:InfoGen,
  author = {Xu, An and Raginsky, Maxim},
  title = {Information-Theoretic Analysis of Generalization Capability of Learning Algorithms},
  booktitle = {NeurIPS},
  year = {2017},
  note = {Mutual information bounds}
}

% === 7. Neural Tangent Kernel ===
@inproceedings{Jacot:2018:NTK,
  author = {Jacot, Arthur and Gabriel, Fran\c cois and Hongler, Clément},
  title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2018},
  note = {Kernel view of wide-network behavior}
}

% === 8. Margin / Norm-Based Bounds ===

@article{Bartlett:1998:MarginComplexity,
  author = {Bartlett, Peter L.},
  title = {The Sample Complexity of Pattern Classification with Margin},
  journal = {IEEE Transactions on Information Theory},
  year = {1998},
  volume = {44},
  number = {2},
  pages = {525--536},
  note = {Margin bounds in classification}
}
% === 9. Algorithmic Robustness / Robustness-based bounds ===
@inproceedings{XuMannor:2010:RobustnessGeneralization,
  author    = {Huan Xu and Shie Mannor},
  title     = {Robustness and Generalization},
  booktitle = {Proceedings of the 23rd Annual Conference on Learning Theory (COLT)},
  year      = {2010},
  url       = {https://arxiv.org/abs/1005.2243},
  note      = {arXiv:1005.2243; derives generalization bounds from algorithmic robustness}
}

@article{XuCaramanisMannor:2009:RobustRegSVM,
  author  = {Huan Xu and Constantine Caramanis and Shie Mannor},
  title   = {Robustness and Regularization of Support Vector Machines},
  journal = {Journal of Machine Learning Research},
  year    = {2009},
  volume  = {10},
  pages   = {1485--1510},
  url     = {http://www.jmlr.org/papers/volume10/xu09a/xu09a.pdf},
  note    = {Connects robustness notions to regularization; useful canonical reference}
}

% === 10. Online learning / Regret bounds (Weighted Majority, multiplicative weights) ===
@article{LittlestoneWarmuth:1994:WeightedMajority,
  author  = {Nick Littlestone and Manfred K. Warmuth},
  title   = {The Weighted Majority Algorithm},
  journal = {Information and Computation},
  year    = {1994},
  volume  = {108},
  number  = {2},
  pages   = {212--261},
  doi     = {10.1006/inco.1994.1009},
  note    = {Classic multiplicative-weights algorithm and regret bounds}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@book{McArtneyInterpolation2003,
title = "Interpolation and Approximation by Polynomials",
abstract = "This book covers the main topics concerned with interpolation and approximation by polynomials. This subject can be traced back to the precalculus era but has enjoyed most of its growth and development since the end of the nineteenth century and is still a lively and flourishing part of mathematics. In addition to coverage of univariate interpolation and approximation, the text includes material on multivariate interpolation and multivariate numerical integration, a generalization of the Bernstein polynomials that has not previously appeared in book form, and a greater coverage of Peano kernel theory than is found in most textbooks. There are many worked examples and each section ends with a number of carefully selected problems that extend the student's understanding of the text.George Phillips has lectured and researched in mathematics at the University of St. Andrews, Scotland. His most recent book, Two Millenia of Mathematics: From Archimedes to Gauss (Springer 2000), received enthusiastic reviews in the USA, Britain and Canada. He is well known for his clarity of writing and his many contributions as a researcher in approximation theory.",
author = "Phillips, {George McArtney}",
year = "2003",
doi = "10.1007/b97417",
language = "English",
isbn = "978-0-387-00215-6",
series = "CMS Books in Mathematics",
publisher = "Springer",
address = "Netherlands",
edition = "1",
}
@phdthesis{piera_sample_2005,
	type = {Doctoral thesis},
	title = {Sample {Covariance} {Based} {Parameter} {Estimation} {For} {Digital} {Communications}},
	copyright = {ADVERTIMENT. L'accés als continguts d'aquesta tesi doctoral i la seva utilització ha de respectar els drets de la persona autora. Pot ser utilitzada per a consulta o estudi personal, així com en activitats o materials d'investigació i docència en els termes establerts a l'art. 32 del Text Refós de la Llei de Propietat Intel·lectual (RDL 1/1996). Per altres utilitzacions es requereix l'autorització prèvia i expressa de la persona autora. En qualsevol cas, en la utilització dels seus continguts caldrà indicar de forma clara el nom i cognoms de la persona autora i el títol de la tesi doctoral. No s'autoritza la seva reproducció o altres formes d'explotació efectuades amb finalitats de lucre ni la seva comunicació pública des d'un lloc aliè al servei TDX. Tampoc s'autoritza la presentació del seu contingut en una finestra o marc aliè a TDX (framing). Aquesta reserva de drets afecta tant als continguts de la tesi com als seus resums i índexs.},
	url = {https://upcommons.upc.edu/handle/2117/94206},
	abstract = {DOI: 10.5821/dissertation-2117-94206},
	language = {eng},
	urldate = {2025-05-25},
	school = {Universitat Politècnica de Catalunya},
	author = {Piera, Villares and Javier, Nemesio},
	month = oct,
	year = {2005},
	doi = {10.5821/dissertation-2117-94206},
	note = {Accepted: 2011-04-12T15:27:01Z
ISBN: 9788468995571
Publication Title: TDX (Tesis Doctorals en Xarxa)},
	keywords = {Àrees temàtiques de la UPC::Enginyeria de la telecomunicació, Comunicacions digitals, estimació, estimadors quadràtics, filtre de kalman, màxima versemblança, nuisance parameters, processament d'arrays, sincronització},
	file = {Full Text PDF:C\:\\Users\\Admin\\Zotero\\storage\\MTVQVFLR\\Piera and Javier - 2005 - Sample Covariance Based Parameter Estimation For D.pdf:application/pdf},
}

@misc{MkayPretenceSignalStatistics1993,
	author = {Steven M. Kay},
	year = {1993},
	title = {Fundamentals of statistical signal processing: estimation theory {\textbar} {Guide} books {\textbar} {ACM} {Digital} {Library}},
	url = {https://dl.acm.org/doi/10.5555/151045},
	urldate = {2025-05-25},
	file = {Fundamentals of statistical signal processing\: estimation theory | Guide books | ACM Digital Library:C\:\\Users\\Admin\\Zotero\\storage\\FKAFWPMZ\\151045.html:text/html},
}


@article{achlioptas_stochastic_nodate,
	title = {Stochastic {Gradient} {Descent} in {Theory} and {Practice}},
	language = {en},
	journal = {Lecture note, Stanford's AI},
	author = {Achlioptas, Panos},
	file = {Achlioptas - Stochastic Gradient Descent in Theory and Practice.pdf:C\:\\Users\\Admin\\Zotero\\storage\\7B7AKI5E\\Achlioptas - Stochastic Gradient Descent in Theory and Practice.pdf:application/pdf},
}
@book{LehmannCasella_theory_1998,
	author = {E. L. Lehmann , George Casella},
	address = {New York},
	series = {Springer {Texts} in {Statistics}},
	title = {Theory of {Point} {Estimation}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-0-387-98502-2},
	url = {http://link.springer.com/10.1007/b98854},
	language = {en},
	urldate = {2025-05-25},
	publisher = {Springer-Verlag},
	year = {1998},
	doi = {10.1007/b98854},
	keywords = {average, Likelihood, minimum, Point Estimation, statistical inference, statistics, Variance},
}
@article{Alquier_2024,
   title={User-friendly Introduction to PAC-Bayes Bounds},
   volume={17},
   ISSN={1935-8245},
   url={http://dx.doi.org/10.1561/2200000100},
   DOI={10.1561/2200000100},
   number={2},
   journal={Foundations and Trends® in Machine Learning},
   publisher={Now Publishers},
   author={Alquier, Pierre},
   year={2024},
   pages={174–303} }
@misc{belkin2018understanddeeplearningneed,
      title={To understand deep learning we need to understand kernel learning}, 
      author={Mikhail Belkin and Siyuan Ma and Soumik Mandal},
      year={2018},
      eprint={1802.01396},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1802.01396}, 
}
@misc{mei2020generalizationerrorrandomfeatures,
      title={The generalization error of random features regression: Precise asymptotics and double descent curve}, 
      author={Song Mei and Andrea Montanari},
      year={2020},
      eprint={1908.05355},
      archivePrefix={arXiv},
      primaryClass={math.ST},
      url={https://arxiv.org/abs/1908.05355}, 
}
@article{mei2019generalization,
  title={The generalization error of random features regression: Precise asymptotics and double descent curve},
  author={Mei, Song and Montanari, Andrea},
  journal={arXiv preprint arXiv:1908.05355},
  year={2019}
}
@misc{advani2017highdimensionaldynamicsgeneralizationerror,
      title={High-dimensional dynamics of generalization error in neural networks}, 
      author={Madhu S. Advani and Andrew M. Saxe},
      year={2017},
      eprint={1710.03667},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1710.03667}, 
}

@misc{power2022grokkinggeneralizationoverfittingsmall,
      title={Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets}, 
      author={Alethea Power and Yuri Burda and Harri Edwards and Igor Babuschkin and Vedant Misra},
      year={2022},
      eprint={2201.02177},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2201.02177}, 
}
@misc{soudry2024implicitbiasgradientdescent,
      title={The Implicit Bias of Gradient Descent on Separable Data}, 
      author={Daniel Soudry and Elad Hoffer and Mor Shpigel Nacson and Suriya Gunasekar and Nathan Srebro},
      year={2024},
      eprint={1710.10345},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1710.10345}, 
}
@misc{wei2022emergentabilitieslargelanguage,
      title={Emergent Abilities of Large Language Models}, 
      author={Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
      year={2022},
      eprint={2206.07682},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2206.07682}, 
}
@misc{vandeven2024continuallearningcatastrophicforgetting,
      title={Continual Learning and Catastrophic Forgetting}, 
      author={Gido M. van de Ven and Nicholas Soures and Dhireesha Kudithipudi},
      year={2024},
      eprint={2403.05175},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.05175}, 
}
@article{Manin_2024,
   title={Homotopy Theoretic and Categorical Models of Neural Information Networks},
   volume={Volume 6 (2024)},
   ISSN={2631-4444},
   url={http://dx.doi.org/10.46298/compositionality-6-4},
   DOI={10.46298/compositionality-6-4},
   journal={Compositionality},
   publisher={Centre pour la Communication Scientifique Directe (CCSD)},
   author={Manin, Yuri and Marcolli, Matilde},
   year={2024},
   month=sep }
@misc{hu2021modelcomplexitydeeplearning,
      title={Model Complexity of Deep Learning: A Survey}, 
      author={Xia Hu and Lingyang Chu and Jian Pei and Weiqing Liu and Jiang Bian},
      year={2021},
      eprint={2103.05127},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2103.05127}, 
}
@misc{luo2024investigatingimpactmodelcomplexity,
      title={Investigating the Impact of Model Complexity in Large Language Models}, 
      author={Jing Luo and Huiyuan Wang and Weiran Huang},
      year={2024},
      eprint={2410.00699},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.00699}, 
}
@misc{barceló2020modelinterpretabilitylenscomputational,
      title={Model Interpretability through the Lens of Computational Complexity}, 
      author={Pablo Barceló and Mikaël Monet and Jorge Pérez and Bernardo Subercaseaux},
      year={2020},
      eprint={2010.12265},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2010.12265}, 
}
@inbook{Molnar_2020,
   title={Quantifying Model Complexity via Functional Decomposition for Better Post-hoc Interpretability},
   ISBN={9783030438234},
   ISSN={1865-0937},
   url={http://dx.doi.org/10.1007/978-3-030-43823-4_17},
   DOI={10.1007/978-3-030-43823-4_17},
   booktitle={Machine Learning and Knowledge Discovery in Databases},
   publisher={Springer International Publishing},
   author={Molnar, Christoph and Casalicchio, Giuseppe and Bischl, Bernd},
   year={2020},
   pages={193–204} 
}
@misc{janik2021complexitydeepneuralnetworks,
      title={Complexity for deep neural networks and other characteristics of deep feature representations}, 
      author={Romuald A. Janik and Przemek Witaszczyk},
      year={2021},
      eprint={2006.04791},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.04791}, 
}
@misc{adlam2020understandingdoubledescentrequires,
      title={Understanding Double Descent Requires a Fine-Grained Bias-Variance Decomposition}, 
      author={Ben Adlam and Jeffrey Pennington},
      year={2020},
      eprint={2011.03321},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2011.03321}, 
}
@Article{electronics13020416,
AUTHOR = {Barbierato, Enrico and Gatti, Alice},
TITLE = {The Challenges of Machine Learning: A Critical Review},
JOURNAL = {Electronics},
VOLUME = {13},
YEAR = {2024},
NUMBER = {2},
ARTICLE-NUMBER = {416},
URL = {https://www.mdpi.com/2079-9292/13/2/416},
ISSN = {2079-9292},
ABSTRACT = {The concept of learning has multiple interpretations, ranging from acquiring knowledge or skills to constructing meaning and social development. Machine Learning (ML) is considered a branch of Artificial Intelligence (AI) and develops algorithms that can learn from data and generalize their judgment to new observations by exploiting primarily statistical methods. The new millennium has seen the proliferation of Artificial Neural Networks (ANNs), a formalism able to reach extraordinary achievements in complex problems such as computer vision and natural language recognition. In particular, designers claim that this formalism has a strong resemblance to the way the biological neurons operate. This work argues that although ML has a mathematical/statistical foundation, it cannot be strictly regarded as a science, at least from a methodological perspective. The main reason is that ML algorithms have notable prediction power although they cannot necessarily provide a causal explanation about the achieved predictions. For example, an ANN could be trained on a large dataset of consumer financial information to predict creditworthiness. The model takes into account various factors like income, credit history, debt, spending patterns, and more. It then outputs a credit score or a decision on credit approval. However, the complex and multi-layered nature of the neural network makes it almost impossible to understand which specific factors or combinations of factors the model is using to arrive at its decision. This lack of transparency can be problematic, especially if the model denies credit and the applicant wants to know the specific reasons for the denial. The model’s “black box” nature means it cannot provide a clear explanation or breakdown of how it weighed the various factors in its decision-making process. Secondly, this work rejects the belief that a machine can simply learn from data, either in supervised or unsupervised mode, just by applying statistical methods. The process of learning is much more complex, as it requires the full comprehension of a learned ability or skill. In this sense, further ML advancements, such as reinforcement learning and imitation learning denote encouraging similarities to similar cognitive skills used in human learning.},
DOI = {10.3390/electronics13020416}
}
@misc{lipton2018troublingtrendsmachinelearning,
      title={Troubling Trends in Machine Learning Scholarship}, 
      author={Zachary C. Lipton and Jacob Steinhardt},
      year={2018},
      eprint={1807.03341},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1807.03341}, 
}
@article{kapoor2022leakage,
  title = {Leakage and the Reproducibility Crisis in ML-based Science},
  author = {Kapoor, Sayash and Narayanan, Arvind},
  journal = {arXiv preprint arXiv:2207.07048},
  year = {2022}
}
@article{lipton2016mythos,
  title = {The Mythos of Model Interpretability},
  author = {Lipton, Zachary C.},
  journal = {Queue},
  volume = {16},
  number = {3},
  pages = {31--57},
  year = {2018}
}
@inproceedings{doshi2017towards,
  title = {Towards A Rigorous Science of Interpretable Machine Learning},
  author = {Doshi-Velez, Finale and Kim, Been},
  booktitle = {arXiv preprint arXiv:1702.08608},
  year = {2017}
}
@article{molnar2020general,
  title = {General Pitfalls of Model-Agnostic Interpretation Methods for Machine Learning Models},
  author = {Molnar, Christoph and König, Gunnar and Herbinger, Julia and Freiesleben, Timo and Dandl, Susanne and Scholbeck, Christian A. and Casalicchio, Giuseppe and Grosse-Wentrup, Moritz and Bischl, Bernd},
  journal = {arXiv preprint arXiv:2007.04131},
  year = {2020}
}
@article{romer2015mathiness,
  title = {Mathiness in the Theory of Economic Growth},
  author = {Romer, Paul M.},
  journal = {American Economic Review},
  volume = {105},
  number = {5},
  pages = {89--93},
  year = {2015}
}
@misc{syll2024postreal,
  title = {Post-real economics — a severe case of mathiness},
  author = {Syll, Lars P{\aa}lsson},
  year = {2024},
  howpublished = {Blog post, Heterodox Economic Blogs}
}
@techreport{dreyfus1965alchemy,
  title = {Alchemy and Artificial Intelligence},
  author = {Dreyfus, Hubert L.},
  institution = {RAND Corporation},
  number = {P-3244},
  year = {1965},
  url = {https://www.rand.org/pubs/papers/P3244.html}
}
@book{dreyfus1972what,
  title = {What Computers Can't Do: A Critique of Artificial Reason},
  author = {Dreyfus, Hubert L.},
  publisher = {Harper \& Row},
  year = {1972},
  isbn = {0060110821}
}
@book{dreyfus1986mind,
  title = {Mind Over Machine: The Power of Human Intuition and Expertise in the Era of the Computer},
  author = {Dreyfus, Hubert L. and Dreyfus, Stuart E.},
  publisher = {Free Press},
  year = {1986},
  isbn = {0029080606}
}
@book{suchman1987plans,
  title = {Plans and Situated Actions: The Problem of Human--Machine Communication},
  author = {Suchman, Lucy A.},
  publisher = {Cambridge University Press},
  year = {1987},
  isbn = {0521388473}
}
@article{brooks1991intelligence,
  title = {Intelligence Without Representation},
  author = {Brooks, Rodney A.},
  journal = {Artificial Intelligence},
  volume = {47},
  pages = {139--159},
  year = {1991},
  doi = {10.1016/0004-3702(91)90053-M},
  url = {https://people.csail.mit.edu/brooks/papers/representation.pdf}
}
@article{searle1980minds,
  title = {Minds, Brains, and Programs},
  author = {Searle, John R.},
  journal = {Behavioral and Brain Sciences},
  volume = {3},
  number = {3},
  pages = {417--457},
  year = {1980}
}
@incollection{mccarthy1969philosophical,
  title = {Some Philosophical Problems from the Standpoint of Artificial Intelligence},
  author = {McCarthy, John and Hayes, Patrick J.},
  booktitle = {Machine Intelligence 4},
  editor = {B. Meltzer and D. Michie},
  pages = {463--502},
  publisher = {Edinburgh University Press},
  year = {1969}
}
@article{harnad1990symbol,
  title = {The Symbol Grounding Problem},
  author = {Harnad, Stevan},
  journal = {Physica D: Nonlinear Phenomena},
  volume = {42},
  pages = {335--346},
  year = {1990},
  doi = {10.1016/0167-2789(90)90087-6}
}

> **Contemporary AI/ML critiques & related theory**

```bibtex
@book{pearl2009causality,
  title = {Causality: Models, Reasoning, and Inference},
  author = {Pearl, Judea},
  edition = {2nd},
  publisher = {Cambridge University Press},
  year = {2009},
  isbn = {9780521895606},
  doi = {10.1017/CBO9780511803161}
}
@misc{marcus2018deep,
  title = {Deep Learning: A Critical Appraisal},
  author = {Marcus, Gary},
  year = {2018},
  howpublished = {arXiv preprint arXiv:1801.00631},
  url = {https://arxiv.org/abs/1801.00631}
}
@misc{sutton2019bitter,
  title = {The Bitter Lesson},
  author = {Sutton, Richard S.},
  year = {2019},
  howpublished = {Web essay / blog post},
  url = {https://www.incompleteideas.net/IncIdeas/BitterLesson.html}
}
@misc{dubey2022activationfunctionsdeeplearning,
      title={Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark}, 
      author={Shiv Ram Dubey and Satish Kumar Singh and Bidyut Baran Chaudhuri},
      year={2022},
      eprint={2109.14545},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2109.14545}, 
}
@misc{zhang2017understandingdeeplearningrequires,
      title={Understanding deep learning requires rethinking generalization}, 
      author={Chiyuan Zhang and Samy Bengio and Moritz Hardt and Benjamin Recht and Oriol Vinyals},
      year={2017},
      eprint={1611.03530},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1611.03530}, 
}
@misc{truong2025rademachercomplexitybasedgeneralizationbounds,
      title={On Rademacher Complexity-based Generalization Bounds for Deep Learning}, 
      author={Lan V. Truong},
      year={2025},
      eprint={2208.04284},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2208.04284}, 
}
@misc{nagarajan2021uniformconvergenceunableexplain,
      title={Uniform convergence may be unable to explain generalization in deep learning}, 
      author={Vaishnavh Nagarajan and J. Zico Kolter},
      year={2021},
      eprint={1902.04742},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1902.04742}, 
}
@online{tunali2019empirical,
  author       = {Onur Tunali},
  title        = {Empirical Rademacher Complexity and Its Implications to Deep Learning},
  year         = {2019},
  month        = feb,
  day          = {1},
  note         = {Accessed: 2025-08-29},
  url          = {https://www.onurtunali.com/ml/2019/02/01/empirical-rademacher-complexity-and-its-implications-to-deep-learning.html}
}
@inproceedings{bartlett2005local,
  author    = {Peter L. Bartlett and Shahar Mendelson},
  title     = {Empirical minimization},
  booktitle = {Probability Theory and Related Fields},
  year      = {2005},
  volume    = {135},
  number    = {3},
  pages     = {311--334},
  doi       = {10.1007/s00440-005-0460-0}
}

@inproceedings{bousquet2002stability,
  author    = {Olivier Bousquet and André Elisseeff},
  title     = {Stability and Generalization},
  booktitle = {Journal of Machine Learning Research},
  year      = {2002},
  volume    = {2},
  pages     = {499--526}
}
@inproceedings{neyshabur2015norm,
  author    = {Behnam Neyshabur and Ryota Tomioka and Nathan Srebro},
  title     = {Norm-Based Capacity Control in Neural Networks},
  booktitle = {Conference on Learning Theory (COLT)},
  year      = {2015},
  pages     = {1376--1401}
}
@inproceedings{bartlett2017spectrally,
  author    = {Peter L. Bartlett and Dylan J. Foster and Matus Telgarsky},
  title     = {Spectrally-normalized margin bounds for neural networks},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2017},
  pages     = {6241--6250}
}
@book{Wegener1987,
  author    = {Wegener, Ingo},
  title     = {The Complexity of Boolean Functions},
  publisher = {John Wiley \& Sons},
  year      = {1987},
  address   = {Chichester, UK},
}

@article{MiltersenRadhakrishnanWegener2005,
  author  = {Miltersen, Peter B. and Radhakrishnan, Jaikumar and Wegener, Ingo},
  title   = {On Converting {CNF} to {DNF}},
  journal = {Theoretical Computer Science},
  volume  = {347},
  number  = {1–2},
  pages   = {325–335},
  year    = {2005},
}

@article{DarwicheMarquis2002,
  author  = {Darwiche, Adnan and Marquis, Pierre},
  title   = {A Knowledge Compilation Map},
  journal = {Journal of Artificial Intelligence Research},
  volume  = {17},
  pages   = {229–264},
  year    = {2002},
}

@misc{huang2025samplecomplexityrepresentationability,
      title={Sample Complexity and Representation Ability of Test-time Scaling Paradigms}, 
      author={Baihe Huang and Shanda Li and Tianhao Wu and Yiming Yang and Ameet Talwalkar and Kannan Ramchandran and Michael I. Jordan and Jiantao Jiao},
      year={2025},
      eprint={2506.05295},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2506.05295}, 
}
@misc{wegel2025samplecomplexitysemisupervisedmultiobjective,
      title={On the sample complexity of semi-supervised multi-objective learning}, 
      author={Tobias Wegel and Geelon So and Junhyung Park and Fanny Yang},
      year={2025},
      eprint={2508.17152},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2508.17152}, 
}
@inproceedings{Mei2022TowardsBridging,
  title        = {Towards Bridging Sample Complexity and Model Capacity},
  author       = {Shibin Mei and Chenglong Zhao and Shengchao Yuan and Bingbing Ni},
  booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
  series       = {AAAI'22 Technical Tracks},
  volume       = {36},
  number       = {2},
  pages        = {1972--1980},
  year         = {2022},
  month        = jun,
  doi          = {10.1609/aaai.v36i2.20092},
}
@article{Balcan2010TrueSampleComplexity,
  title        = {The true sample complexity of active learning},
  author       = {Balcan, Maria-Florina and Hanneke, Steve and Wortman Vaughan, Jennifer},
  journal      = {Machine Learning},
  volume       = {80},
  number       = {2–3},
  pages        = {111--139},
  year         = {2010},
  publisher    = {Springer},
  doi          = {10.1007/s10994-010-5174-y},
  url          = {https://link.springer.com/article/10.1007/s10994-010-5174-y},
}
@article{compmodel2016,
  author       = {Michael, Recorla},
  title        = {Computational Modeling of the Mind: What Role for Mental Representation?},
  year         = {2016},
  journal      = {UCLA Philosophy},
  url          = {https://philosophy.ucla.edu/wp-content/uploads/2016/08/Computation-Modeling.pdf},
  note         = {Accessed: 2025-01-04}
}
@inproceedings{Cristianini2000AnIT,
  title={An Introduction to Support Vector Machines and Other Kernel-based Learning Methods},
  author={Nello Cristianini and John Shawe-Taylor},
  year={2000},
  url={https://api.semanticscholar.org/CorpusID:60486887}
}
@inproceedings{wang2017research,
  author       = {Huibing Wang and Jinbo Xiong and Zhiqiang Yao and Mingwei Lin and Jun Ren},
  title        = {Research Survey on Support Vector Machine},
  booktitle    = {Proceedings of the 10th EAI International Conference on Mobile Multimedia Communications (MOBIMEDIA)},
  year         = {2017},
  month        = dec,
  publisher    = {EAI},
  doi          = {10.4108/eai.13-7-2017.2270596}
}
@inproceedings{domingos_unified_2000,
  author    = {Pedro Domingos},
  title     = {A Unified Bias-Variance Decomposition and its Applications},
  booktitle = {Proceedings of the 17th International Conference on Machine Learning (ICML-2000)},
  year      = {2000},
  pages     = {231--238},
  publisher = {Morgan Kaufmann},
  url       = {https://homes.cs.washington.edu/~pedrod/papers/mlc00a.pdf}
}

@inproceedings{domingos_unified_aaai_2000,
  author    = {Pedro Domingos},
  title     = {A Unified Bias-Variance Decomposition for Zero-One and Squared Loss},
  booktitle = {Proceedings of the 17th National Conference on Artificial Intelligence (AAAI-2000)},
  year      = {2000},
  pages     = {564--569},
  publisher = {AAAI Press},
  url       = {https://homes.cs.washington.edu/~pedrod/papers/aaai00.pdf}
}
@misc{transtrum2025egaddoubledescentexplained,
      title={eGAD! double descent is explained by Generalized Aliasing Decomposition}, 
      author={Mark K. Transtrum and Gus L. W. Hart and Tyler J. Jarvis and Jared P. Whitehead},
      year={2025},
      eprint={2408.08294},
      archivePrefix={arXiv},
      primaryClass={math.ST},
      url={https://arxiv.org/abs/2408.08294}, 
}
@misc{liu2021kernelregressionhighdimensions,
      title={Kernel regression in high dimensions: Refined analysis beyond double descent}, 
      author={Fanghui Liu and Zhenyu Liao and Johan A. K. Suykens},
      year={2021},
      eprint={2010.02681},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2010.02681}, 
}
@misc{allerbo2025changingkerneltrainingleads,
      title={Changing the Kernel During Training Leads to Double Descent in Kernel Regression}, 
      author={Oskar Allerbo},
      year={2025},
      eprint={2311.01762},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2311.01762}, 
}
@misc{pezeshki2021multiscalefeaturelearningdynamics,
      title={Multi-scale Feature Learning Dynamics: Insights for Double Descent}, 
      author={Mohammad Pezeshki and Amartya Mitra and Yoshua Bengio and Guillaume Lajoie},
      year={2021},
      eprint={2112.03215},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2112.03215}, 
}
@inproceedings{
brellmann2024on,
title={On Double Descent in Reinforcement Learning with {LSTD} and Random Features},
author={David Brellmann and Elo{\"\i}se Berthier and David Filliat and Goran Frehse},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=9RIbNmx984}
}
@misc{nakkiran2019moredata,
  title = {More Data Can Hurt for Linear Regression: Sample-wise Double Descent},
  author = {Nakkiran, Preetum},
  year = {2019},
  howpublished = {arXiv:1912.07242},
  note = {arXiv preprint},
  url = {https://arxiv.org/abs/1912.07242}
}
@article{mei2019randomfeatures,
  title = {The generalization error of random features regression: Precise asymptotics and double descent curve},
  author = {Mei, Song and Montanari, Andrea},
  journal = {arXiv preprint},
  year = {2019},
  howpublished = {arXiv:1908.05355},
  url = {https://arxiv.org/abs/1908.05355}
}
@misc{heckel2020early,
  title = {Early Stopping in Deep Networks: Double Descent and How to Eliminate it},
  author = {Heckel, Rebekka and Yilmaz, Murat},
  year = {2020},
  howpublished = {arXiv preprint},
  note = {discusses epoch-wise double descent and mitigation strategies},
  url = {https://arxiv.org/abs/2007.10099}
}
@misc{nakkiran2020regularization,
  title = {Optimal Regularization Can Mitigate Double Descent (OpenReview entry)},
  author = {Nakkiran, Preetum and Venkat, Prayaag and Kakade, Sham and Ma, Tengyu},
  year = {2020},
  howpublished = {OpenReview forum submission},
  url = {https://openreview.net/forum?id=7R7fAoUygoa}
}
@misc{lee2022vctheoreticalexplanationdouble,
      title={VC Theoretical Explanation of Double Descent}, 
      author={Eng Hock Lee and Vladimir Cherkassky},
      year={2022},
      eprint={2205.15549},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2205.15549}, 
}
@article{Belkin_2020,
   title={Two Models of Double Descent for Weak Features},
   volume={2},
   ISSN={2577-0187},
   url={http://dx.doi.org/10.1137/20M1336072},
   DOI={10.1137/20m1336072},
   number={4},
   journal={SIAM Journal on Mathematics of Data Science},
   publisher={Society for Industrial & Applied Mathematics (SIAM)},
   author={Belkin, Mikhail and Hsu, Daniel and Xu, Ji},
   year={2020},
   month=jan, pages={1167–1180} }
@article{Yang_2024,
   title={Dropout drops double descent},
   volume={7},
   ISSN={2520-8764},
   url={http://dx.doi.org/10.1007/s42081-024-00242-5},
   DOI={10.1007/s42081-024-00242-5},
   number={2},
   journal={Japanese Journal of Statistics and Data Science},
   publisher={Springer Science and Business Media LLC},
   author={Yang, Tian-Le and Suzuki, Joe},
   year={2024},
   month=mar, pages={615–632} }
@misc{spiess2023doublesingledescentcausal,
      title={Double and Single Descent in Causal Inference with an Application to High-Dimensional Synthetic Control}, 
      author={Jann Spiess and Guido Imbens and Amar Venugopal},
      year={2023},
      eprint={2305.00700},
      archivePrefix={arXiv},
      primaryClass={econ.EM},
      url={https://arxiv.org/abs/2305.00700}, 
}
@misc{nakkiran2021optimalregularizationmitigatedouble,
      title={Optimal Regularization Can Mitigate Double Descent}, 
      author={Preetum Nakkiran and Prayaag Venkat and Sham Kakade and Tengyu Ma},
      year={2021},
      eprint={2003.01897},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2003.01897}, 
}
@misc{zhang2024manipulatingsparsedoubledescent,
      title={Manipulating Sparse Double Descent}, 
      author={Ya Shi Zhang},
      year={2024},
      eprint={2401.10686},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.10686}, 
}
@article{cherkassky2024understand,
  title = {To understand double descent, we need to understand VC theory},
  author = {Vladimir Cherkassky and Eng Hock Lee},
  journal = {Neural Networks},
  volume = {169},
  pages = {242--256},
  year = {2024},
  doi = {10.1016/j.neunet.2023.10.014},
  url = {https://doi.org/10.1016/j.neunet.2023.10.014}
}
@misc{nakkiran2019datahurtlinearregression,
      title={More Data Can Hurt for Linear Regression: Sample-wise Double Descent}, 
      author={Preetum Nakkiran},
      year={2019},
      eprint={1912.07242},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1912.07242}, 
}
@misc{luo2023doubledescentdiscrepancytask,
      title={Double Descent of Discrepancy: A Task-, Data-, and Model-Agnostic Phenomenon}, 
      author={Yifan Luo and Bin Dong},
      year={2023},
      eprint={2305.15907},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.15907}, 
}
@inproceedings{NEURIPS2024_2d43f7a6,
 author = {Li, Xinyue and Sonthalia, Rishi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {25510--25560},
 publisher = {Curran Associates, Inc.},
 title = {Least Squares Regression Can Exhibit Under-Parameterized Double Descent},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/2d43f7a61b57f83619f82c971e4bddc0-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}
@misc{harzli2023doubledescentcurvesneuralnetworks,
      title={Double-descent curves in neural networks: a new perspective using Gaussian processes}, 
      author={Ouns El Harzli and Bernardo Cuenca Grau and Guillermo Valle-Pérez and Ard A. Louis},
      year={2023},
      eprint={2102.07238},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2102.07238}, 
}
@misc{curth2023uturndoubledescentrethinking,
      title={A U-turn on Double Descent: Rethinking Parameter Counting in Statistical Learning}, 
      author={Alicia Curth and Alan Jeffares and Mihaela van der Schaar},
      year={2023},
      eprint={2310.18988},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2310.18988}, 
}
@misc{ruder_overview_2017,
	title = {An overview of gradient descent optimization algorithms},
	url = {http://arxiv.org/abs/1609.04747},
	doi = {10.48550/arXiv.1609.04747},
	abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
	urldate = {2025-06-28},
	publisher = {arXiv},
	author = {Ruder, Sebastian},
	month = jun,
	year = {2017},
	note = {arXiv:1609.04747 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Admin\\Zotero\\storage\\QIVZWHXY\\Ruder - 2017 - An overview of gradient descent optimization algor.pdf:application/pdf;Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\KYVXYR4N\\1609.html:text/html},
}

@misc{zhang_gradient_2019,
	title = {Gradient {Descent} based {Optimization} {Algorithms} for {Deep} {Learning} {Models} {Training}},
	url = {http://arxiv.org/abs/1903.03614},
	doi = {10.48550/arXiv.1903.03614},
	abstract = {In this paper, we aim at providing an introduction to the gradient descent based optimization algorithms for learning deep neural network models. Deep learning models involving multiple nonlinear projection layers are very challenging to train. Nowadays, most of the deep learning model training still relies on the back propagation algorithm actually. In back propagation, the model variables will be updated iteratively until convergence with gradient descent based optimization algorithms. Besides the conventional vanilla gradient descent algorithm, many gradient descent variants have also been proposed in recent years to improve the learning performance, including Momentum, Adagrad, Adam, Gadam, etc., which will all be introduced in this paper respectively.},
	urldate = {2025-06-28},
	publisher = {arXiv},
	author = {Zhang, Jiawei},
	month = mar,
	year = {2019},
	note = {arXiv:1903.03614 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Admin\\Zotero\\storage\\RIRNXDED\\Zhang - 2019 - Gradient Descent based Optimization Algorithms for.pdf:application/pdf;Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\4CLP7N7A\\1903.html:text/html},
}
% Academic Papers on LLMs and AGI Expectations

@article{feng2024how,
  title={How Far Are We From AGI: Are LLMs All We Need?},
  author={Feng, Tao and Jin, Chuanyang and Liu, Jingyu and Zhu, Kunlun and Tu, Haoqin and Cheng, Zirui and Lin, Guanyu and You, Jiaxuan},
  journal={arXiv preprint arXiv:2405.10313},
  year={2024},
  month={May},
  url={https://arxiv.org/abs/2405.10313}
}

@article{llm2025agi,
  title={Large language models for artificial general intelligence (AGI): A survey of foundational principles and approaches},
  author={Alhassan Mumuni, Fuseini Mumuni},
  journal={arXiv preprint arXiv:2501.03151},
  year={2025},
  month={January},
  url={https://arxiv.org/abs/2501.03151}
}

@article{agiprediction2025,
  title={AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities},
  author={Fabrizio Davide, Pietro Torre, Leonardo Ercolani, Andrea Gaggioli},
  journal={arXiv preprint arXiv:2412.09385},
  year={2024},
  month={December},
  url={https://arxiv.org/abs/2412.09385},
  note={Published April 2025 based on December 2024 preprint}
}

@article{zhao2023survey,
  title={Large Language Models: A Survey},
  author={Zhao, Wayne Xin and others},
  journal={arXiv preprint arXiv:2402.06196},
  year={2024},
  month={February},
  url={https://arxiv.org/abs/2402.06196},
  note={Updated March 2025}
}

@article{apple2024reasoning,
  title={GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models},
  author={Mirzadeh, Iman and Alizadeh, Keivan and Shahrokhi, Hooman and Tuzel, Oncel and Bengio, Samy and Farajtabar, Mehrdad},
  journal={Apple Machine Learning Research},
  year={2024},
  month={June},
  note={Research demonstrating fundamental barriers to generalizable reasoning in LLMs}
}

@article{nature2024agi,
  title={How close is AI to human-level intelligence? Researchers can't agree},
  author={Various},
  journal={Nature},
  year={2024},
  month={December},
  note={Discussion of AGI timelines and LLM limitations with perspectives from Yoshua Bengio and others}
}

@misc{marcus2024llm,
  title={Why LLMs Will Never Be AGI},
  author={Marcus, Gary},
  year={2024},
  month={June},
  howpublished={Online commentary},
  note={Critique of LLM capabilities and AGI expectations}
}

@misc{frewin2024llm,
  title={Why LLMs Will Never Be AGI},
  author={Frewin, Chris},
  year={2024},
  month={September},
  howpublished={Medium/Personal blog},
  note={Software engineer's perspective on LLM-to-AGI expectations}
}

@misc{ccc2025llm,
  title={LLMs Are Not the Path to General Artificial Intelligence},
  author={Bielefield, Roy},
  year={2025},
  month={May},
  publisher={Copyright Clearance Center},
  howpublished={CTO Commentary},
  note={Argues LLMs are foundations for natural language interfaces, not AGI}
}

@misc{hackernews2024embodiment,
  title={Discussion: LLMs, Embodiment, and AGI Potential},
  author={Various contributors},
  year={2024},
  howpublished={Hacker News and online forums},
  note={Community debates on whether LLMs lack necessary corporeal experience for AGI}
}

% Technical Books on AGI

@book{togelius2024agi,
  title={Artificial General Intelligence},
  author={Togelius, Julian},
  year={2024},
  publisher={MIT Press},
  series={MIT Press Essential Knowledge series},
  note={Explores technical approaches to AGI and implications for civilization}
}

@book{goertzel2007agi,
  title={Artificial General Intelligence},
  editor={Goertzel, Ben and Pennachin, Cassio},
  year={2007},
  publisher={Springer},
  series={Cognitive Technologies},
  isbn={978-3-540-23733-4},
  note={Foundational edited volume providing comprehensive AGI coverage}
}

@book{domingos2015master,
  title={The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World},
  author={Domingos, Pedro},
  year={2015},
  publisher={Basic Books},
  isbn={978-0465065707},
  note={Explores the search for a unifying machine learning algorithm that could lead to AGI}
}

@book{kurzweil2005singularity,
  title={The Singularity Is Near: When Humans Transcend Biology},
  author={Kurzweil, Ray},
  year={2005},
  publisher={Viking Press},
  isbn={978-0143037880},
  note={Influential work on AGI speculation and technological singularity}
}

@book{kurzweil1999spiritual,
  title={The Age of Spiritual Machines: When Computers Exceed Human Intelligence},
  author={Kurzweil, Ray},
  year={1999},
  publisher={Viking Press},
  isbn={978-0140282023},
  note={Earlier exploration of machine intelligence reaching human levels}
}
% Foundational Papers on Large Language Models

% The Original Transformer Architecture
@inproceedings{vaswani2017attention,
  title={Attention is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  volume={30},
  pages={5998--6008},
  year={2017},
  publisher={Neural Information Processing Systems},
  url={https://arxiv.org/abs/1706.03762},
  note={NeurIPS 2017. The foundational paper introducing the Transformer architecture}
}

% BERT - Bidirectional Encoder
@inproceedings{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4171--4186},
  year={2019},
  publisher={Association for Computational Linguistics},
  url={https://arxiv.org/abs/1810.04805},
  note={NAACL 2019. Introduced bidirectional pre-training for language representations}
}

% GPT-3 - Large-scale Few-shot Learning
@inproceedings{brown2020language,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020},
  publisher={Neural Information Processing Systems},
  url={https://arxiv.org/abs/2005.14165},
  note={NeurIPS 2020. Introduced GPT-3 with 175B parameters demonstrating few-shot learning}
}

% Comprehensive LLM Surveys
@article{zhao2023llmsurvey,
  title={A Survey of Large Language Models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023},
  month={March},
  url={https://arxiv.org/abs/2303.18223},
  note={Comprehensive survey covering pre-training, adaptation, utilization, and evaluation of LLMs. Updated through March 2025}
}

@article{zhao2024llmsurvey2,
  title={Large Language Models: A Survey},
  author={Zhao, Wayne Xin and others},
  journal={arXiv preprint arXiv:2402.06196},
  year={2024},
  month={February},
  url={https://arxiv.org/abs/2402.06196},
  note={Reviews prominent LLM families (GPT, LLaMA, PaLM) and discusses techniques for building and augmenting LLMs}
}

% GPT-2
@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019},
  url={https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf},
  note={Introduced GPT-2, demonstrating zero-shot task transfer}
}

% GPT-1
@article{radford2018improving,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  journal={OpenAI blog},
  year={2018},
  url={https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf},
  note={The original GPT paper introducing generative pre-training}
}

% T5
@article{raffel2020exploring,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020},
  url={https://arxiv.org/abs/1910.10683},
  note={Introduced T5, treating every NLP task as a text-to-text problem}
}

% LLaMA
@article{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023},
  url={https://arxiv.org/abs/2302.13971},
  note={Meta's open foundation models ranging from 7B to 65B parameters}
}

% LLaMA 2
@article{touvron2023llama2,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023},
  url={https://arxiv.org/abs/2307.09288},
  note={Updated and improved LLaMA models with better performance}
}

% PaLM
@article{chowdhery2022palm,
  title={PaLM: Scaling Language Modeling with Pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022},
  url={https://arxiv.org/abs/2204.02311},
  note={Google's 540B parameter model demonstrating breakthrough capabilities}
}

% InstructGPT / RLHF
@article{ouyang2022training,
  title={Training Language Models to Follow Instructions with Human Feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022},
  url={https://arxiv.org/abs/2203.02155},
  note={Introduced RLHF for aligning language models with human preferences}
}

% Chain-of-Thought Prompting
@article{wei2022chain,
  title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022},
  url={https://arxiv.org/abs/2201.11903},
  note={NeurIPS 2022. Demonstrated how step-by-step reasoning improves LLM performance}
}

% Scaling Laws
@article{kaplan2020scaling,
  title={Scaling Laws for Neural Language Models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020},
  url={https://arxiv.org/abs/2001.08361},
  note={Established empirical scaling laws for language model performance}
}

% Chinchilla (Optimal Scaling)
@article{hoffmann2022training,
  title={Training Compute-Optimal Large Language Models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022},
  url={https://arxiv.org/abs/2203.15556},
  note={Introduced Chinchilla and revised scaling laws for compute-optimal training}
}

% Constitutional AI
@article{bai2022constitutional,
  title={Constitutional AI: Harmlessness from AI Feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022},
  url={https://arxiv.org/abs/2212.08073},
  note={Anthropic's approach to AI alignment using AI-generated feedback}
}

% Papers Arguing Against LLMs Being AGI or Achieving AGI-Level Capabilities

% The Foundational Critique - Stochastic Parrots
@inproceedings{bender2021dangers,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages={610--623},
  year={2021},
  publisher={ACM},
  doi={10.1145/3442188.3445922},
  url={https://dl.acm.org/doi/10.1145/3442188.3445922},
  note={Foundational critique arguing LLMs statistically mimic text without real understanding, introducing the "stochastic parrot" metaphor}
}

% Apple Research on Reasoning Limitations
@article{mirzadeh2024gsm,
  title={GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models},
  author={Mirzadeh, Iman and Alizadeh, Keivan and Shahrokhi, Hooman and Tuzel, Oncel and Bengio, Samy and Farajtabar, Mehrdad},
  journal={arXiv preprint arXiv:2410.05229},
  year={2024},
  month={October},
  url={https://arxiv.org/abs/2410.05229},
  note={Demonstrates fundamental barriers to generalizable reasoning, with complete performance collapse on complex problems}
}

@inproceedings{apple2025illusion,
title = {The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity},
booktitle = {NeurIPS},
author = {Parshin Shojaee, Iman Mirzadeh, Keivan Alizadeh, Maxwell Horton, Samy Bengio and Mehrdad Farajtabar},
year = {2025},
URL = {https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf}
}

% Comprehensive Survey of LLM Limitations
@article{zhang2025lllms,
  title={LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models},
  author={Zhang, Yijun and others},
  journal={arXiv preprint arXiv:2505.19240},
  year={2025},
  month={May},
  url={https://arxiv.org/abs/2505.19240},
  note={Semi-automated review of 14,648 papers on LLM limitations from 2022-2024, covering reasoning failures, hallucinations, and multilingual capabilities}
}

% Primer on LLM Limitations
@article{llm2024primer,
  title={A Primer on Large Language Models and their Limitations},
  author={Sandra Johnson, David Hyland-Wood},
  journal={arXiv preprint arXiv:2412.04503},
  year={2024},
  month={December},
  url={https://arxiv.org/abs/2412.04503},
  note={Argues LLMs lack self-monitoring (phenomenal consciousness) and internal updatable models of their environment}
}

% Gary Marcus's Critiques
@misc{marcus2023elegant,
  title={Elegant and powerful new result that seriously undermines large language models},
  author={Marcus, Gary},
  year={2023},
  month={September},
  howpublished={Marcus on AI (Substack)},
  url={https://garymarcus.substack.com/p/elegant-and-powerful-new-result-that},
  note={Critique of LLM capabilities with proposed disclaimer: "All facts presented by Generative AI—even those that are true—are fictitious"}
}

@misc{marcus2025knockout,
  title={A knockout blow for LLMs?},
  author={Marcus, Gary},
  year={2025},
  month={June},
  howpublished={Marcus on AI (Substack)},
  url={https://garymarcus.substack.com/p/a-knockout-blow-for-llms},
  note={Discusses Apple research showing LLMs overanthropomorphize reasoning traces and produce incorrect answers despite correct-appearing reasoning}
}


% Formal Analysis - Communications of the ACM
@article{cacm2025knockout,
  title={A Knockout Blow for LLMs?},
  author={Gary Marcus},
  journal={Communications of the ACM},
  year={2025},
  month={June},
  url={https://cacm.acm.org/blogcacm/a-knockout-blow-for-llms/},
  note={Discusses devastating Apple research paper on LLM reasoning limitations, noting advocates are "partly conceding the blow"}
}

% Systematic Evaluation of Critical Limitations
@article{limitgen2025,
  title={Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers},
  author={Zhijian Xu, Yilun Zhao and Manasi Patwardhan and Lovekesh Vig and Arman Cohan},
  journal={arXiv preprint arXiv:2507.02694},
  year={2025},
  month={July},
  url={https://arxiv.org/abs/2507.02694},
  note={Presents LimitGen benchmark and taxonomy of limitation types in AI research, evaluating LLMs' meta-cognitive capabilities}
}

@article{limgen2024,
  title={LimGen: Probing the LLMs for Generating Suggestive Limitations of Research Papers},
  author={Abdur Rahman Bin Md Faizullah, Ashok Urlana, Rahul Mishra},
  journal={arXiv preprint arXiv:2403.15529},
  year={2024},
  month={March},
  url={https://arxiv.org/abs/2403.15529},
  note={Shows summarization-specific models fail to generate limitations due to their training objectives}
}

% Philosophical and Theoretical Critiques
@misc{njii2024agi,
  title={How Close is AGI Actually? Why LLMs Alone Will Not Get us to AGI},
  author={Tom Villani},
  year={2024},
  month={July},
  publisher={New Jersey Innovation Institute},
  url={https://www.njii.com/2024/07/why-llms-alone-will-not-get-us-to-agi/},
  note={Argues recent NLP advancements, while remarkable, do not constitute a path to AGI}
}

@misc{alignmentforum2025,
  title={Beware General Claims about "Generalizable Reasoning Capabilities" (of Modern AI Systems)},
  author={AI Alignment Forum},
  year={2025},
  month={June},
  howpublished={AI Alignment Forum},
  url={https://www.alignmentforum.org/posts/5uw26uDdFbFQgKzih/beware-general-claims-about-generalizable-reasoning},
  note={Discusses historical arguments from Gary Marcus and statistical learning theorists about limitations of neural network architectures}
}

% Popular Science and Commentary
@misc{towardsai2023parrots,
  title={Stochastic Parrots: A Novel Look at Large Language Models and Their Limitations},
  author={Muhammad Saad Uddin},
  year={2023},
  month={April},
  publisher={Towards AI},
  url={https://towardsai.net/p/machine-learning/stochastic-parrots-a-novel-look-at-large-language-models-and-their-limitations},
  note={Models are not capable of true reasoning or understanding, prone to errors and biases, and perpetuate stereotypes}
}

@misc{friedman2024inference,
  title={Understanding Inference and the "Stochastic Parrot" in Large Language Models},
  author={Friedman, Dave},
  year={2024},
  month={December},
  howpublished={Personal blog (Substack)},
  url={https://davefriedman.substack.com/p/understanding-inference-and-the-stochastic},
  note={Argues LLMs are sophisticated pattern-matchers devoid of understanding, reasoning, or intentionality}
}

% Counter-Arguments and Debates
@article{lesswrong2024parrot,
  title={The Stochastic Parrot Hypothesis is debatable for the last generation of LLMs},
  author={Quentin FEUILLADE--MONTIXI, Pierre Peigné},
  year={2024},
  howpublished={LessWrong},
  url={https://www.lesswrong.com/posts/HxRjHq3QG8vcYy4yy/the-stochastic-parrot-hypothesis-is-debatable-for-the-last},
  note={Provides examples where LLMs fail basic inference with novel information, supporting the stochastic parrot critique}
}

% Critical Response to Stochastic Parrots
@article{baan2021slodderwetenschap,
  title={The Slodderwetenschap (Sloppy Science) of Stochastic Parrots -- A Plea for Science to NOT take the Route Advocated by Gebru and Bender},
  author={Baan, Joris},
  journal={arXiv preprint arXiv:2101.10098},
  year={2021},
  month={January},
  url={https://arxiv.org/abs/2101.10098},
  note={Counter-argument to the Stochastic Parrots paper, criticizing its methodology and ethics}
}
@misc{wang2025capabilitiesgpt5multimodalmedical,
      title={Capabilities of GPT-5 on Multimodal Medical Reasoning}, 
      author={Shansong Wang and Mingzhe Hu and Qiang Li and Mojtaba Safari and Xiaofeng Yang},
      year={2025},
      eprint={2508.08224},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2508.08224}, 
}
@misc{openai_gpt5_2025,
  author       = {{OpenAI}},
  title        = {Introducing GPT-5},
  year         = {2025},
  howpublished = {\url{https://openai.com/gpt-5/}},
  note         = {OpenAI product/technical announcement (Aug 7, 2025).}
}

@misc{openai_inside_gpt5_2025,
  author       = {{OpenAI}},
  title        = {Inside GPT-5 for Work},
  year         = {2025},
  howpublished = {\url{https://cdn.openai.com/pdf/inside-gpt-5-for-work.pdf}},
  note         = {Technical overview / PDF from OpenAI.}
}
@misc{guo_deepseek_r1_2025,
  author       = {D. Guo and others},
  title        = {DeepSeek-R1: Incentivizing Reasoning Capability in LLMs},
  year         = {2025},
  howpublished = {arXiv preprint arXiv:2501.12948},
  url          = {https://arxiv.org/abs/2501.12948},
  note         = {Introduces DeepSeek-R1 and DeepSeek-R1-Zero; RL-based reasoning training.}
}

@misc{deepseek_hf_2025,
  author       = {{DeepSeek AI / Hugging Face}},
  title        = {deepseek-ai/DeepSeek-R1 (model card)},
  year         = {2025},
  howpublished = {\url{https://huggingface.co/deepseek-ai/DeepSeek-R1}},
  note         = {Model page and downloads (weights / details).}
}
@misc{anthropic_claude3_modelcard_2024,
  author       = {{Anthropic}},
  title        = {The Claude 3 Model Family: Opus, Sonnet, Haiku (Model Card)},
  year         = {2024},
  howpublished = {\url{https://www-cdn.anthropic.com/.../Model_Card_Claude_3.pdf}},
  note         = {Official Claude 3 model card (PDF) from Anthropic.}
}

@misc{anthropic_tracing_thoughts_2025,
  author       = {{Anthropic Research}},
  title        = {Tracing the Thoughts of a Large Language Model},
  year         = {2025},
  howpublished = {\url{https://www.anthropic.com/research/tracing-thoughts-language-model}},
  note         = {Research blog/papers on interpretability and internal features.}
}
@article{touvron_llama_2023,
  author    = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth{\'e}e Lacroix and Baptiste Rozi{\`e}re and Naman Goyal and Eric Hambro and Faisal Azhar and Aur{\'e}lien Rodriguez and Armand Joulin and {\'E}douard Grave and Guillaume Lample},
  title     = {LLaMA: Open and Efficient Foundation Language Models},
  journal   = {arXiv preprint arXiv:2302.13971},
  year      = {2023},
  url       = {https://arxiv.org/abs/2302.13971}
}
@inproceedings{chowdhery_palm_2022,
  author    = {Amitabh Chowdhery and others},
  title     = {PaLM: Scaling Language Modeling with Pathways},
  year      = {2022},
  howpublished = {arXiv preprint arXiv:2204.02311},
  url       = {https://arxiv.org/abs/2204.02311}
}

@misc{anil_palm2_2023,
  author    = {R. Anil and others},
  title     = {PaLM 2 Technical Report},
  year      = {2023},
  howpublished = {arXiv preprint arXiv:2305.10403},
  url       = {https://arxiv.org/abs/2305.10403}
}
@misc{jiang_mistral7b_2023,
  author    = {A. Q. Jiang and others},
  title     = {Mistral 7B},
  year      = {2023},
  howpublished = {arXiv preprint arXiv:2310.06825},
  url       = {https://arxiv.org/abs/2310.06825},
  note      = {Mistral AI release (7B model) and technical description.}
}

@misc{mistral_blog_2023,
  author    = {{Mistral AI}},
  title     = {Announcing Mistral 7B},
  year      = {2023},
  howpublished = {\url{https://mistral.ai/news/announcing-mistral-7b}},
  note      = {Official release blog and model card.}
}

@book{bostrom_superintelligence_2014,
  author    = {Nick Bostrom},
  title     = {Superintelligence: Paths, Dangers, Strategies},
  publisher = {Oxford University Press},
  year      = {2014},
  isbn      = {978-0199678112}
}

@book{barrat_our_final_invention_2013,
  author    = {James Barrat},
  title     = {Our Final Invention: Artificial Intelligence and the End of the Human Era},
  publisher = {Thomas Dunne Books},
  year      = {2013},
  isbn      = {978-0-312-62237-4}
}

@book{birch_edge_of_sentience_2024,
  author    = {Jonathan Birch},
  title     = {The Edge of Sentience: Risk and Precaution in Humans, Other Animals, and AI},
  publisher = {Oxford University Press},
  year      = {2024},
  isbn      = {978-0-19-287042-1}
}

@book{yudkowsky_soares_anyone_builds_2025,
  author    = {Eliezer Yudkowsky and Nate Soares},
  title     = {If Anyone Builds It, Everyone Dies: Why Superhuman AI Would Kill Us All},
  publisher = {Hachette Book Group},
  year      = {2025},
  isbn      = {9780316595643}
}

@book{hao_empire_of_ai_2025,
  author    = {Karen Hao},
  title     = {Empire of AI: Dreams and Nightmares in Sam Altman's OpenAI},
  publisher = {Penguin Press},
  year      = {2025},
  isbn      = {978-0593657508}
}

@article{cui_risk_taxonomy_mitigation_2024,
  author    = {Tianyu Cui and Yanling Wang and Chuanpu Fu and Yong Xiao and Sijia Li and Xinhao Deng and Yunpeng Liu and Qinglin Zhang and Ziyi Qiu and Peiyang Li and Zhixing Tan and Junwu Xiong and Xinyu Kong and Zujie Wen and Ke Xu and Qi Li},
  title     = {Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems},
  journal   = {arXiv preprint},
  year      = {2024},
  howpublished = {arXiv:2401.05778},
  url       = {https://arxiv.org/abs/2401.05778}
}

@article{bucknall_dori_hacohen_current_near_term_2022,
  author    = {Benjamin S. Bucknall and Shiri Dori-Hacohen},
  title     = {Current and Near-Term AI as a Potential Existential Risk Factor},
  journal   = {arXiv preprint},
  year      = {2022},
  howpublished = {arXiv:2209.10604},
  url       = {https://arxiv.org/abs/2209.10604}
}

@article{faroldi_risk_and_agency_2024,
  author    = {Federico L. G. Faroldi},
  title     = {Risk and Artificial General Intelligence},
  journal   = {AI \& Society},
  volume    = {40},
  number    = {4},
  pages     = {2541--2549},
  year      = {2025},
  doi       = {10.1007/s00146-024-02004-z}
}

@article{koessler_schuett_2023_risk_assessment_agi_companies,
  author    = {Leonie Koessler and Jonas Schuett},
  title     = {Risk assessment at AGI companies: A review of popular risk assessment techniques from other safety-critical industries},
  journal   = {arXiv preprint},
  year      = {2023},
  howpublished = {arXiv:2307.08823},
  url       = {https://arXiv.org/abs/2307.08823}
}
@article{bubeck_sparks_agi_2023,
  author    = {S{\'e}bastien Bubeck and Varun Chandrasekaran and Ronen Eldan and Johannes Gehrke and Eric Horvitz and Ece Kamar and Peter Lee and Yin Tat Lee and Yuanzhi Li and Scott Lundberg and Harsha Nori and Hamid Palangi and Marco Tulio Ribeiro and Yi Zhang},
  title     = {Sparks of Artificial General Intelligence: Early experiments with GPT-4},
  journal   = {arXiv preprint},
  year      = {2023},
  howpublished = {arXiv:2303.12712},
  url       = {https://arxiv.org/abs/2303.12712}
}

@article{mumuni_survey_llms_for_agi_2025,
  author       = {Alhassan Mumuni and Fuseini Mumuni},
  title        = {Large Language Models for Artificial General Intelligence: A Survey of Foundational Principles and Approaches},
  journal      = {arXiv preprint},
  year         = {2025},
  howpublished = {arXiv:2501.03151},
  url           = {https://arxiv.org/abs/2501.03151}
}

@article{shang_ai_native_memory_2024,
  author       = {Jingbo Shang and Zai Zheng and Jiale Wei and Xiang Ying and Felix Tao and Mindverse Team},
  title        = {AI-native Memory: A Pathway from LLMs Towards AGI},
  journal      = {arXiv preprint},
  year         = {2024},
  howpublished = {arXiv:2406.18312},
  url           = {https://arxiv.org/abs/2406.18312}
}

@article{llm_cognitive_capabilities_evidence_2024,
  author    = {David Ilić,  Gilles E. Gignac},
  title     = {Evidence of interrelated cognitive-like capabilities in large language models: Indications of artificial general intelligence or achievement?},
  journal   = {Intelligence},
  volume    = {106},
  pages     = {101858},
  year      = {2024},
  doi       = {10.1016/j.intell.2024.101858},
  url       = {https://doi.org/10.1016/j.intell.2024.101858}
}

@article{goertzel_generative_ai_vs_agi_2023,
  author    = {Ben Goertzel},
  title     = {Generative AI vs. AGI: The Cognitive Strengths and Weaknesses of Modern LLMs},
  journal   = {arXiv preprint},
  year      = {2023},
  howpublished = {arXiv:2309.10371},
  url         = {https://arxiv.org/abs/2309.10371}
}



@article{llms_assessment_for_singularity_2025,
  author    = {Ryunosuke Ishizaki, Mahito Sugiyama},
  title     = {Large language models: assessment for singularity},
  journal   = {AI \& Society},
  year      = {2025},
  note      = {Open access},
  url       = {https://link.springer.com/article/10.1007/s00146-025-02271-4}
}

@article{altmeyer_position_stop_unsci_agi_claims_2024,
  author    = {Patrick Altmeyer and Andrew M. Demetriou and Antony Bartlett and Cynthia C. S. Liem},
  title     = {Position: Stop Making Unscientific AGI Performance Claims},
  journal   = {arXiv preprint},
  year      = {2024},
  howpublished = {arXiv:2402.03962},
  url         = {https://arxiv.org/abs/2402.03962}
}
@article{turing1950computing,
  author = {Turing, Alan M.},
  title = {Computing Machinery and Intelligence},
  journal = {Mind},
  volume = {59},
  number = {236},
  pages = {433--460},
  year = {1950},
  note = {Classic proposal of the ``Imitation Game'' (Turing Test).}
}

@misc{dartmouth1955proposal,
  author = {McCarthy, John and Minsky, Marvin and Rochester, Nathaniel and Shannon, Claude E.},
  title = {A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence},
  howpublished = {Technical proposal (Aug 31, 1955); reprinted/retrospective in AI Magazine 2006},
  year = {1955},
  note = {Founding proposal that coined the term ``artificial intelligence''.}
}

@book{russell2010aima,
  author = {Russell, Stuart and Norvig, Peter},
  title = {Artificial Intelligence: A Modern Approach},
  edition = {3},
  publisher = {Prentice Hall},
  year = {2010},
  note = {Standard textbook that frames AI via agents and a taxonomy of definitions/goals.}
}

@misc{stanford2018ai,
  author = {{Stanford Encyclopedia of Philosophy}},
  title = {Artificial Intelligence},
  howpublished = {\url{https://plato.stanford.edu/entries/artificial-intelligence/}},
  year = {2018},
  note = {A well-rounded survey of history, proposed definitions, and philosophy-of-AI debates.}
}

@book{penrose1989emperor,
  author = {Penrose, Roger},
  title = {The Emperor's New Mind: Concerning Computers, Minds, and the Laws of Physics},
  publisher = {Oxford University Press},
  year = {1989},
  note = {Philosophical argument against the possibility of full computational emulation of human consciousness.}
}


@article{floridi2004philosophy,
  author = {Floridi, Luciano},
  title = {From the Philosophy of Information to an Information Ethics},
  journal = {Philosophy \& Technology},
  year = {2004},
  note = {Places AI debates within a broader `information' philosophical framework.}
}
@book{mccarthy1987general,
  author = {McCarthy, John},
  title = {Generality in Artificial Intelligence},
  publisher = {Communications of the ACM},
  year = {1987},
  note = {A key voice emphasizing the distinction between domain-limited AI ('weak') and the aspiration toward general AI ('strong').}
}
@mastersthesis{Oygarden2019,
  author       = {Øygarden, Even},
  title        = {What is Intelligence? A Proposed Framework of Four Different Concepts of Intelligence},
  school       = {University of Agder},
  year         = {2019},
  type         = {Master’s thesis},
  supervisor   = {Einar Duenger Bøhn},
  url          = {https://uia.brage.unit.no/uia-xmlui/bitstream/handle/11250/2632728/%C3%98ygarden%2C%20Even.pdf?sequence=1},
}
@misc{merriam,
  author       = {{Merriam-Webster}},
  title        = {Artificial},
  year         = {2025},
  url          = {https://www.merriam-webster.com/dictionary/artificial},
  note         = {Accessed: 2025-10-01}
}

@misc{cambridge,
  author       = {{Cambridge Dictionary}},
  title        = {Artificial},
  year         = {2025},
  url          = {https://dictionary.cambridge.org/dictionary/english/artificial},
  note         = {Accessed: 2025-10-01}
}

@misc{oxford,
  author       = {{Oxford Learner's Dictionaries}},
  title        = {Artificial},
  year         = {2025},
  url          = {https://www.oxfordlearnersdictionaries.com/definition/english/artificial},
  note         = {Accessed: 2025-10-01}
}

@misc{dictionary-com,
  author       = {{Dictionary.com}},
  title        = {Artificial},
  year         = {2025},
  url          = {https://www.dictionary.com/browse/artificial},
  note         = {Accessed: 2025-10-01}
}

@misc{justia,
  author       = {{Justia Legal Dictionary}},
  title        = {Artificial},
  year         = {2025},
  url          = {https://dictionary.justia.com/artificial},
  note         = {Accessed: 2025-10-01}
}

@misc{etymology,
  author       = {{Merriam-Webster Etymology}},
  title        = {Artificial (etymology)},
  year         = {2025},
  url          = {https://www.merriam-webster.com/dictionary/artificial},
  note         = {Accessed: 2025-10-01}
}

@article{bianchini,
  author    = {Bianchini, Andrea},
  title     = {On the Notion of Artificial in the Age of Synthetic Biology and AI},
  journal   = {Foundations of Science},
  year      = {2021},
  doi       = {10.1007/s10699-021-09799-w}
}

@misc{nasa,
  author       = {{NASA}},
  title        = {What is Artificial Intelligence?},
  year         = {2023},
  url          = {https://www.nasa.gov/what-is-artificial-intelligence/},
  note         = {Accessed: 2025-10-01}
}

@misc{ibm,
  author       = {{IBM}},
  title        = {Artificial Intelligence},
  year         = {2024},
  url          = {https://www.ibm.com/think/topics/artificial-intelligence},
  note         = {Accessed: 2025-10-01}
}

@article{legg-hutter,
  author    = {Legg, Shane and Hutter, Marcus},
  title     = {Universal Intelligence: A Definition of Machine Intelligence},
  journal   = {Minds and Machines},
  year      = {2007},
  volume    = {17},
  pages     = {391--444},
  doi       = {10.1007/s11023-007-9079-x}
}

@inproceedings{goertzel,
  author    = {Goertzel, Ben},
  title     = {Artificial General Intelligence: Concept, State of the Art, and Future Prospects},
  booktitle = {Artificial General Intelligence},
  publisher = {Springer},
  year      = {2014},
  pages     = {1--48}
}
@book{simon1969_sciences,
  author    = {Herbert A. Simon},
  title     = {The Sciences of the Artificial},
  edition   = {1},
  year      = {1969},
  publisher = {MIT Press},
  address   = {Cambridge, MA},
  isbn      = {978-0262190510},
}

@book{haugeland1985_veryidea,
  author    = {John Haugeland},
  title     = {Artificial Intelligence: The Very Idea},
  year      = {1985},
  publisher = {MIT Press},
  address   = {Cambridge, MA},
  isbn      = {978-0262580953},
}

@book{boden1987_artificial_natural_man_expanded,
  author    = {Margaret A. Boden},
  title     = {Artificial Intelligence and Natural Man},
  edition   = {2, expanded},
  year      = {1987},
  publisher = {MIT Press / Basic Books},
  address   = {Cambridge, MA / New York},
  isbn13    = {978-0262521239},
}

@book{boden1996_philosophy_artificial_life,
  editor    = {Margaret A. Boden},
  title     = {The Philosophy of Artificial Life},
  series    = {Oxford Readings in Philosophy},
  year      = {1996},
  publisher = {Oxford University Press},
  address   = {New York / Oxford},
  isbn10    = {0198751559},
  isbn13    = {978-0198751557},
}

@book{boden2019_from_fingers_to_digits,
  author    = {Margaret A. Boden and Ernest A. Edmonds},
  title     = {From Fingers to Digits: An Artificial Aesthetic},
  year      = {2019},
  publisher = {MIT Press},
  address   = {Cambridge, MA},
  isbn13    = {978-0262039628},
}

@incollection{onyeukaziri2022_natural_artificial,
  author    = {Justin Nnaemeka Onyeukaziri},
  title     = {Artificial Intelligence and the Notions of the “Natural” and the “Artificial”},
  booktitle = {Journal of Data Analysis},
  volume    = {17},
  number    = {4},
  pages     = {101--116},
  year      = {2022},
}

@book{boden1990_philosophy_of_ai,
  editor    = {Margaret A. Boden},
  title     = {The Philosophy of Artificial Intelligence},
  year      = {1990},
  publisher = {Oxford University Press},
  address   = {New York / Oxford},
  isbn10    = {0198248547},  
  isbn13    = {978-0198248545}
}
@book{putnam1988_representation,
  author    = {Hilary Putnam},
  title     = {Representation and Reality},
  year      = {1988},
  publisher = {MIT Press},
  address   = {Cambridge, MA},
  isbn      = {978-0262660594}
}

@book{fodor1975_languageofthought,
  author    = {Jerry A. Fodor},
  title     = {The Language of Thought},
  year      = {1975},
  publisher = {Harvard University Press},
  address   = {Cambridge, MA},
  isbn      = {978-0674510302}
}

@book{churchland1986_neurophilosophy,
  author    = {Patricia S. Churchland},
  title     = {Neurophilosophy: Toward a Unified Science of the Mind-Brain},
  year      = {1986},
  publisher = {MIT Press},
  address   = {Cambridge, MA},
  isbn      = {978-0262031165}
}

@book{dennett1978_brainstorms,
  author    = {Daniel C. Dennett},
  title     = {Brainstorms: Philosophical Essays on Mind and Psychology},
  year      = {1978},
  publisher = {MIT Press},
  address   = {Cambridge, MA},
  isbn      = {978-0262540377}
}

@book{dennett1991_consciousness,
  author    = {Daniel C. Dennett},
  title     = {Consciousness Explained},
  year      = {1991},
  publisher = {Little, Brown and Company},
  address   = {Boston},
  isbn      = {978-0316180665}
}

@book{scheutz2002_computationalism,
  editor    = {Matthias Scheutz},
  title     = {Computationalism: New Directions},
  year      = {2002},
  publisher = {MIT Press},
  address   = {Cambridge, MA},
  isbn      = {978-0262692843}
}

@article{dodig2012_infocomputationalism,
  author    = {Gordana Dodig-Crnkovic},
  title     = {Info-Computationalism and Morphological Computing of Informational Structures},
  journal   = {Information},
  year      = {2012},
  volume    = {3},
  number    = {2},
  pages     = {204--218},
  doi       = {10.3390/info3020204}
}

@article{gauvrit2015_algorithmic_cognition,
  author    = {Nicolas Gauvrit and Hector Zenil and Per {Tegnér}},
  title     = {The Information-Theoretic and Algorithmic Approach to Human, Animal and Artificial Cognition},
  journal   = {arXiv preprint},
  year      = {2015},
  eprint    = {1501.04242},
  archivePrefix = {arXiv}
}

@article{copeland2018_evolution_computationalism,
  author    = {Jack Copeland and Diane Proudfoot},
  title     = {From Computer Metaphor to Computational Modeling: The Evolution of Computationalism},
  journal   = {Minds and Machines},
  year      = {2018},
  volume    = {28},
  number    = {4},
  pages     = {515--542},
  doi       = {10.1007/s11023-018-9468-3}
}
@article{searle1980_mindsbrains,
  author    = {John R. Searle},
  title     = {Minds, Brains, and Programs},
  journal   = {Behavioral and Brain Sciences},
  year      = {1980},
  volume    = {3},
  number    = {3},
  pages     = {417--457},
  doi       = {10.1017/S0140525X00005756}
}

@book{searle1992_rediscovery,
  author    = {John R. Searle},
  title     = {The Rediscovery of the Mind},
  year      = {1992},
  publisher = {MIT Press},
  address   = {Cambridge, MA},
  isbn      = {978-0262691549}
}

@book{penrose1989_emperorsmind,
  author    = {Roger Penrose},
  title     = {The Emperor’s New Mind: Concerning Computers, Minds, and the Laws of Physics},
  year      = {1989},
  publisher = {Oxford University Press},
  address   = {Oxford},
  isbn      = {978-0198519737}
}

@book{penrose1994_shadows,
  author    = {Roger Penrose},
  title     = {Shadows of the Mind: A Search for the Missing Science of Consciousness},
  year      = {1994},
  publisher = {Oxford University Press},
  address   = {Oxford},
  isbn      = {978-0198539780}
}

@book{nagel2012_mindandcosmos,
  author    = {Thomas Nagel},
  title     = {Mind and Cosmos: Why the Materialist Neo-Darwinian Conception of Nature Is Almost Certainly False},
  year      = {2012},
  publisher = {Oxford University Press},
  address   = {New York},
  isbn      = {978-0199919758}
}

@article{muller2025_symbolgrounding,
  author    = {Vincent C. Müller},
  title     = {Symbol Grounding in Computational Systems: A Paradox of Intentions},
  journal   = {arXiv preprint},
  year      = {2025},
  eprint    = {2505.00002},
  archivePrefix = {arXiv}
}

@article{laforte1998_godel_computationalism,
  author    = {Geoffrey LaForte},
  title     = {Why Gödel’s Theorem Cannot Refute Computationalism},
  journal   = {Minds and Machines},
  year      = {1998},
  volume    = {8},
  number    = {4},
  pages     = {577--593},
  doi       = {10.1023/A:1008320429318}
}
@misc{legg2007collectiondefinitionsintelligence,
      title={A Collection of Definitions of Intelligence}, 
      author={Shane Legg and Marcus Hutter},
      year={2007},
      eprint={0706.3639},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/0706.3639}, 
}
@book{gardner1983frames,
  title     = {Frames of Mind: The Theory of Multiple Intelligences},
  author    = {Howard Gardner},
  year      = {1983},
  publisher = {Basic Books},
  address   = {New York}
}
@article{sapkota2025agentic,
  title={AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications, and Challenges},
  author={Sapkota, Ranjan and Rosenthal, Konstantinos I. and Karkee, Manoj},
  journal={arXiv preprint arXiv:2505.10468},
  year={2025},
  url={https://arxiv.org/abs/2505.10468}
}

@article{derouiche2025agentic,
  title={Agentic AI Frameworks: Architectures, Protocols, and Design Challenges},
  author={Derouiche, Hana and Brahmi, Zaki and Mazeni, Haithem},
  journal={arXiv preprint arXiv:2508.10146},
  year={2025},
  url={https://arxiv.org/abs/2508.10146}
}

@article{schneider2025generative,
  title={Generative to Agentic AI: Survey, Conceptualization, and Future Directions},
  author={Schneider, J.},
  journal={arXiv preprint arXiv:2504.18875},
  year={2025},
  url={https://arxiv.org/abs/2504.18875}
}

@article{wei2025agentic,
  title={From AI for Science to Agentic Science: A Survey on Autonomous Scientific Discovery},
  author={Wei, Jiaqi and others},
  journal={arXiv preprint arXiv:2508.14111},
  year={2025},
  url={https://arxiv.org/abs/2508.14111}
}

@article{raza2025trism,
  title={TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems},
  author={Raza, Shaina and Sapkota, Ranjan and Karkee, Manoj and Emmanouilidis, Christos},
  journal={arXiv preprint arXiv:2506.04133},
  year={2025},
  url={https://arxiv.org/abs/2506.04133}
}
@book{penrose1997artificial,
  title={Artificial Intelligence Versus Natural Intelligence},
  author={Penrose, Roger and Severino, Emanuele},
  year={1997},
  publisher={Springer},
  address={Dordrecht, Netherlands},
  url={https://link.springer.com/book/10.1007/978-3-030-85480-5}
}

@article{peyres2025mathematics,
  title={The Mathematics of Artificial Intelligence},
  author={Peyré, Gabriel},
  journal={arXiv preprint arXiv:2501.10465},
  year={2025},
  url={https://arxiv.org/abs/2501.10465}
}

@article{kutyniok2022mathematics,
  title={The Mathematics of Artificial Intelligence},
  author={Kutyniok, Gitta},
  journal={arXiv preprint arXiv:2203.08890},
  year={2022},
  url={https://arxiv.org/abs/2203.08890}
}
@article{li2021towards,
  title={Towards Out-Of-Distribution Generalization: A Survey},
  author={Li, Yiyou and Yang, Tianlong and Zhang, Yi and Liu, Yang and Zhao, Pei and Li, Jingtong and Ji, Shuiwang and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2108.13624},
  year={2021}
}

@inproceedings{bahng2022learning,
  title={Learning Representations that Support Extrapolation},
  author={Bahng, Hyojin and Kim, Hyunwoo J. and Yoo, Jaegul},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={1433--1448},
  year={2022},
  organization={PMLR}
}

@article{xu2020how,
  title={How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks},
  author={Xu, Keyulu and Zhang, Mozhi and Li, Jingling and Du, Simon S. and Kawaguchi, Kenji and Jegelka, Stefanie},
  journal={arXiv preprint arXiv:2009.11848},
  year={2020}
}

@incollection{traber2020relex,
  title={ReLEx: Regularisation for Linear Extrapolation in Neural Networks with Rectified Linear Units},
  author={Traber, Thilo and Zech, Andreas and Gessert, Nils and Schlaefer, Alexander},
  booktitle={Artificial Neural Networks and Machine Learning -- ICANN 2020},
  pages={170--182},
  publisher={Springer},
  year={2020},
  doi={10.1007/978-3-030-63799-6_13}
}
@article{huellermeier2021aleatoric,
  title={Aleatoric and Epistemic Uncertainty in Machine Learning: An Introduction to Concepts and Methods},
  author={H{\"u}llermeier, Eyke and Waegeman, Willem},
  journal={Machine Learning},
  volume={110},
  number={3},
  pages={457--506},
  year={2021},
  publisher={Springer},
  doi={10.1007/s10994-021-05946-3}
}

@article{amini2021quantifying,
  title={Quantifying Epistemic Uncertainty in Deep Learning},
  author={Amini, Alexander and Schwarting, Wilko and Soleimany, Ava P. and Rus, Daniela},
  journal={arXiv preprint arXiv:2110.12122},
  year={2021}
}

@article{sensoy2024epistemic,
  title={Is Epistemic Uncertainty Faithfully Represented by Evidential Deep Learning Methods?},
  author={Sensoy, Murat and Kendall, Alex and Esposito, Flavio},
  journal={arXiv preprint arXiv:2402.09056},
  year={2024}
}

@inproceedings{theresa2022information,
  title={Information-Theoretic Analysis of Epistemic Uncertainty in Bayesian Meta-Learning},
  author={Jose, Theresa and Cesa-Bianchi, Nicol{\`o} and Seldin, Yevgeny},
  booktitle={Proceedings of the 39th International Conference on Machine Learning (ICML)},
  pages={11025--11047},
  year={2022},
  organization={PMLR}
}
@article{Harnad90SymbolGrounding,
  author  = {Stevan Harnad},
  title   = {The Symbol Grounding Problem},
  journal = {Physica D: Nonlinear Phenomena},
  volume  = {42},
  number  = {1-3},
  pages   = {335--346},
  year    = {1990},
  doi     = {10.1016/0167-2789(90)90087-6},
  note    = {How symbols in AI can acquire meaning beyond arbitrary manipulation.}
}
@book{Jackson2019,
  author    = {Philip C. Jackson},
  title     = {Introduction to Artificial Intelligence},
  publisher = {Dover Publications},
  year      = {2019},
  edition   = {3},
  isbn      = {0486843076},
  pages     = {544}
}

@book{GoertzelPennachin2006,
  editor    = {Ben Goertzel and Cassio Pennachin},
  title     = {Artificial General Intelligence},
  publisher = {Springer Science \& Business Media},
  year      = {2006},
  isbn      = {9783540686774},
  pages     = {509}
}

@book{Chowdhary2020,
  author    = {K.~R. Chowdhary},
  title     = {Fundamentals of Artificial Intelligence},
  publisher = {Springer},
  year      = {2020},
  isbn      = {978-81-322-3970-3},
  pages     = {716}
}
@article{wolpert1996lack,
  title={The lack of a priori distinctions between learning algorithms},
  author={Wolpert, David H.},
  journal={Neural Computation},
  volume={8},
  number={7},
  pages={1341--1390},
  year={1996},
  doi={10.1162/neco.1996.8.7.1341},
  url={https://direct.mit.edu/neco/article/8/7/1341/6016/The-Lack-of-A-Priori-Distinctions-Between-Learning}
}
@article{shalev-shwartz2010learnability,
  title={Learnability, stability and uniform convergence},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik},
  journal={Journal of Machine Learning Research},
  volume={11},
  pages={2635--2670},
  year={2010},
  url={http://jmlr.org/papers/v11/shalev-shwartz10a.html}
}

@inproceedings{newell1959gps,
  author       = {Newell, Allen and Shaw, J. C. and Simon, Herbert A.},
  title        = {Report on a General Problem-Solving Program},
  booktitle    = {Proceedings of the International Conference on Information Processing},
  pages        = {256--264},
  year         = {1959},
  note         = {Description of GPS (General Problem Solver) — means-ends analysis},
  url          = {https://bitsavers.informatik.uni-stuttgart.de/pdf/rand/ipl/P-1584_Report_On_A_General_Problem-Solving_Program_Feb59.pdf}
}
@article{minsky1961steps,
  author       = {Minsky, Marvin L.},
  title        = {Steps Toward Artificial Intelligence},
  journal      = {Proceedings of the IRE},
  volume       = {49},
  pages        = {8--30},
  year         = {1961},
  note         = {Survey/position paper by Minsky summarizing early AI directions; foundational for symbolic AI},
  doi          = {10.1109/JRPROC.1961.287775},
  url          = {https://courses.csail.mit.edu/6.803/pdf/steps.pdf}
}
@book{minsky1968semantic,
  editor       = {Minsky, Marvin L.},
  title        = {Semantic Information Processing},
  publisher    = {MIT Press},
  address      = {Cambridge, MA},
  year         = {1968},
  note         = {Edited volume containing many classic symbolic AI papers (e.g., McCarthy, Minsky chapters)},
  url          = {https://philpapers.org/rec/MINSIP}
}
@incollection{michalski1969aq,
  author       = {Michalski, Ryszard S.},
  title        = {On the Quasi-Inductive Learning: the AQ approach to rule extraction},
  booktitle    = {Machine Intelligence and Pattern Recognition},
  volume       = {4},
  pages        = {171--197},
  year         = {1969},
  note         = {Early work leading to AQ family of inductive rule-learning algorithms (symbolic rule induction).}
}
@book{quinlan1986induction,
  author       = {Quinlan, J. R.},
  title        = {Induction of Decision Trees},
  publisher    = {Morgan Kaufmann},
  year         = {1986},
  note         = {ID3 / decision-tree induction — classic symbolic learning algorithm widely used in expert systems and symbolic concept learning},
  url          = {https://www.sciencedirect.com/science/article/pii/B9781558602485500092}
}
@article{solomonoff1964formal1,
  author  = {Solomonoff, Ray J.},
  title   = {A Formal Theory of Inductive Inference. Part I},
  journal = {Information and Control},
  volume  = {7},
  number  = {1},
  pages   = {1--22},
  year    = {1964},
  doi     = {10.1016/S0019-9958(64)90223-2},
  url     = {https://doi.org/10.1016/S0019-9958(64)90223-2}
}

@article{solomonoff1964formal2,
  author  = {Solomonoff, Ray J.},
  title   = {A Formal Theory of Inductive Inference. Part II},
  journal = {Information and Control},
  volume  = {7},
  number  = {2},
  pages   = {224--254},
  year    = {1964},
  doi     = {10.1016/S0019-9958(64)90131-7},
  url     = {https://doi.org/10.1016/S0019-9958(64)90131-7}
}
@article{gold1967language,
  author  = {Gold, E. Mark},
  title   = {Language Identification in the Limit},
  journal = {Information and Control},
  volume  = {10},
  number  = {5},
  pages   = {447--474},
  year    = {1967},
  doi     = {10.1016/S0019-9958(67)91165-5},
  url     = {https://doi.org/10.1016/S0019-9958(67)91165-5}
}
@article{newell1958elements,
  author  = {Newell, Allen and Shaw, J.C. and Simon, Herbert A.},
  title   = {Elements of a Theory of Human Problem Solving},
  journal = {Psychological Review},
  volume  = {65},
  number  = {3},
  pages   = {151--166},
  year    = {1958},
  doi     = {10.1037/h0048495},
  url     = {https://doi.org/10.1037/h0048495}
}
@article{vapnik1968uniform,
  author       = {Vapnik, V. N. and Chervonenkis, A. Ya.},
  title        = {The uniform convergence of the frequencies of events to their probabilities},
  journal      = {Doklady Akademii Nauk SSSR},
  volume       = {181},
  number       = {4},
  pages        = {781--783},
  year         = {1968},
  note         = {Translated \& extended in Theory of Probability \& Its Applications 1971},
  url          = {https://mi.mathnet.ru/eng/tvrf/v181/i4/p781}
}

@article{vapnik1971uniform,
  author       = {Vapnik, V. N. and Chervonenkis, A. Ya.},
  title        = {On the Uniform Convergence of Relative Frequencies of Events to their Probabilities},
  journal      = {Theory of Probability \& Its Applications},
  volume       = {16},
  number       = {2},
  pages        = {264--280},
  year         = {1971},
  doi          = {10.1137/1116025},
  url          = {https://link.aip.org/link/?TPR/16/264/1}
}

@article{valiant1984learnable,
  author       = {Valiant, Leslie G.},
  title        = {A Theory of the Learnable},
  journal      = {Communications of the ACM},
  volume       = {27},
  number       = {11},
  pages        = {1134--1142},
  year         = {1984},
  doi          = {10.1145/1968.1972},
  url          = {https://dl.acm.org/doi/10.1145/1968.1972}
}

@article{angluin1988queries,
  author       = {Angluin, Dana},
  title        = {Queries and Concept Learning},
  journal      = {Machine Learning},
  volume       = {2},
  number       = {4},
  pages        = {319--342},
  year         = {1988},
  doi          = {10.1007/BF00116828},
  url          = {https://doi.org/10.1007/BF00116828}
}

@article{computationallimits1989,
  author       = {Angluin, Dana},
  title        = {Computational Limitations on Learning from Examples},
  journal      = {Journal of the ACM},
  volume       = {36},
  number       = {4},
  pages        = {955--981},
  year         = {1989},
  doi          = {10.1145/76322.76335},
  url          = {https://dl.acm.org/doi/10.1145/76322.76335}
}

@article{board1992necessity,
  author       = {Board, Raymond and Pitt, Leonard},
  title        = {On the necessity of Occam algorithms},
  journal      = {Theoretical Computer Science},
  volume       = {100},
  number       = {1},
  pages        = {157--184},
  year         = {1992},
  doi          = {10.1016/0304-3975(92)90367-O},
  url          = {https://doi.org/10.1016/0304-3975(92)90367-O}
}
@book{winograd1972understanding,
  author       = {Winograd, Terry},
  title        = {Understanding Natural Language},
  publisher    = {Academic Press},
  address      = {New York},
  year         = {1972},
  note         = {SHRDLU project — language understanding in a blocks world; core early symbolic NLP system},
  url          = {https://archive.org/details/understandingnat0000wino}
}
@article{cover1965geometrical,
  author  = {Cover, Thomas M.},
  title   = {Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition},
  journal = {IEEE Transactions on Electronic Computers},
  volume  = {EC-14},
  number  = {3},
  pages   = {326--334},
  year    = {1965},
  doi     = {10.1109/PGEC.1965.264137},
  url     = {https://doi.org/10.1109/PGEC.1965.264137}
}
@article{mcallester1999some,
  author  = {McAllester, David A.},
  title   = {Some PAC-Bayesian Theorems},
  journal = {Machine Learning},
  volume  = {37},
  number  = {3},
  pages   = {355--363},
  year    = {1999},
  doi     = {10.1023/A:1007618624809},
  note    = {Early PAC-Bayesian bounds that combine Bayesian ideas with PAC guarantees}  
}
@article{alquier2008pac,
  author  = {Alquier, Pierre},
  title   = {PAC-Bayesian Bounds for Randomized Empirical Risk Minimizers},
  journal = {Mathematical Methods of Statistics},
  volume  = {17},
  number  = {4},
  pages   = {279--304},
  year    = {2008},
  doi     = {10.3103/S1066530708040017},
  note    = {Bounds for randomized predictors; extended PAC-Bayesian theory} 
}
@article{haussler1996rigorous,
  author  = {Haussler, David and Kearns, Michael J. and Seung, H. Sebastian and Tishby, Naftali and others},
  title   = {Rigorous Learning Curve Bounds from Statistical Mechanics},
  journal = {Machine Learning},
  volume  = {25},
  number  = {2--3},
  pages   = {195--236},
  year    = {1996},
  doi     = {10.1007/BF00116991},
  note    = {Connects VC dimension, statistical mechanics, learning curves}  
}
@misc{jeon2025informationtheoreticfoundationsmachinelearning,
      title={Information-Theoretic Foundations for Machine Learning}, 
      author={Hong Jun Jeon and Benjamin Van Roy},
      year={2025},
      eprint={2407.12288},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2407.12288}, 
}
@misc{abreu2025topologicalmachinelearningunreduced,
      title={Topological Machine Learning with Unreduced Persistence Diagrams}, 
      author={Nicole Abreu and Parker B. Edwards and Francis Motta},
      year={2025},
      eprint={2507.07156},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2507.07156}, 
}
@article{BRACHMAN1985171,
title = {An overview of the KL-ONE Knowledge Representation System},
journal = {Cognitive Science},
volume = {9},
number = {2},
pages = {171-216},
year = {1985},
issn = {0364-0213},
doi = {https://doi.org/10.1016/S0364-0213(85)80014-8},
url = {https://www.sciencedirect.com/science/article/pii/S0364021385800148},
author = {Ronald J. Brachman and James G. Schmolze},
abstract = {KL-ONE is a system for representing knowledge in Artificial Intelligence programs. It has been developed and refined over a long period and has been used in both basic research and implemented knowledge-based systems in a number of places in the AI community. Here we present the kernel ideas of KL-ONE, emphasizing its ability to form complex structured descriptions. In addition to detailing all of KL-ONE's description-forming structures, we discuss a bit of the philosophy underlying the system, highlight notions of taxonomy and classification that are central to it, and include an extended example of the use of KL-ONE and its classifier in a recognition task. This research was supported in part by the Defense Advanced Research Projects Agency under Contract N00014-77-C-0378. Views and conclusions contained in this paper are the authors' and should not be interpreted as representing the official opinion or policy of DARPA, the U.S. Government, or any person or agency connected with them.}
}
@article{newell1976computer,
  title={Computer science as empirical inquiry: symbols and search},
  author={Newell, Allen and Simon, Herbert A.},
  journal={Communications of the ACM},
  volume={19},
  number={3},
  pages={113--126},
  year={1976},
  publisher={ACM New York, NY, USA}
}

@article{collins1969retrieval,
  title={Retrieval time from semantic memory},
  author={Collins, Allan M. and Quillian, M. Ross},
  journal={Journal of Verbal Learning and Verbal Behavior},
  volume={8},
  number={2},
  pages={240--247},
  year={1969},
  publisher={Elsevier}
}

@book{shortliffe1976computer,
  title={Computer-Based Medical Consultations: MYCIN},
  author={Shortliffe, Edward Hance},
  year={1976},
  publisher={Elsevier},
  address={New York},
  note={American Elsevier Publishing Company}
}

@article{davis1977production,
  title={Production rules as a representation for a knowledge-based consultation program},
  author={Davis, Randall and Buchanan, Bruce and Shortliffe, Edward},
  journal={Artificial Intelligence},
  volume={8},
  number={1},
  pages={15--45},
  year={1977},
  publisher={Elsevier}
}

@Article{math13111707,
AUTHOR = {Liang, Baoyu and Wang, Yuchen and Tong, Chao},
TITLE = {AI Reasoning in Deep Learning Era: From Symbolic AI to Neural–Symbolic AI},
JOURNAL = {Mathematics},
VOLUME = {13},
YEAR = {2025},
NUMBER = {11},
ARTICLE-NUMBER = {1707},
URL = {https://www.mdpi.com/2227-7390/13/11/1707},
ISSN = {2227-7390},
ABSTRACT = {The pursuit of Artificial General Intelligence (AGI) demands AI systems that not only perceive but also reason in a human-like manner. While symbolic systems pioneered early breakthroughs in logic-based reasoning, such as MYCIN and DENDRAL, they suffered from brittleness and poor scalability. Conversely, modern deep learning architectures have achieved remarkable success in perception tasks, yet continue to fall short in interpretable and structured reasoning. This dichotomy has motivated growing interest in Neural–Symbolic AI, a paradigm that integrates symbolic logic with neural computation to unify reasoning and learning. This survey provides a comprehensive and technically grounded overview of AI reasoning in the deep learning era, with a particular focus on Neural–Symbolic AI. Beyond a historical narrative, we introduce a formal definition of AI reasoning and propose a novel three-dimensional taxonomy that organizes reasoning paradigms by representation form, task structure, and application context. We then systematically review recent advances—including Differentiable Logic Programming, abductive learning, program induction, logic-aware Transformers, and LLM-based symbolic planning—highlighting their technical mechanisms, capabilities, and limitations. In contrast to prior surveys, this work bridges symbolic logic, neural computation, and emergent generative reasoning, offering a unified framework to understand and compare diverse approaches. We conclude by identifying key open challenges such as symbolic–continuous alignment, dynamic rule learning, and unified architectures, and we aim to provide a conceptual foundation for future developments in general-purpose reasoning systems.},
DOI = {10.3390/math13111707}
}
@article{Lindsay1993DENDRALAC,
  title={DENDRAL: A Case Study of the First Expert System for Scientific Hypothesis Formation},
  author={Robert K. Lindsay and Bruce G. Buchanan and Edward A. Feigenbaum and Joshua Lederberg},
  journal={Artif. Intell.},
  year={1993},
  volume={61},
  pages={209-261},
  url={https://api.semanticscholar.org/CorpusID:6929723}
}
@book{clocksin2003programming,
  title={Programming in Prolog: Using the ISO Standard},
  author={Clocksin, William F. and Mellish, Christopher S.},
  year={2003},
  publisher={Springer},
  edition={5th},
  isbn={978-3-540-00678-7}
}
@techreport{colmerauer1972systeme,
  title={Un système de communication homme-machine en français},
  author={Colmerauer, Alain and Kanoui, Henry and Pasero, Robert and Roussel, Philippe},
  year={1972},
  month={October},
  institution={Groupe Intelligence Artificielle, Faculté des Sciences de Luminy, Université Aix-Marseille II},
  address={France},
  type={Rapport préliminaire de fin de contrat IRIA}
}

@incollection{colmerauer1993birth,
  title={The birth of Prolog},
  author={Colmerauer, Alain and Roussel, Philippe},
  booktitle={History of programming languages---II},
  pages={331--367},
  year={1993},
  publisher={ACM},
  note={Originally written November 1992}
}
@article{Cartuyvels_2021,
   title={Discrete and continuous representations and processing in deep learning: Looking forward},
   volume={2},
   ISSN={2666-6510},
   url={http://dx.doi.org/10.1016/j.aiopen.2021.07.002},
   DOI={10.1016/j.aiopen.2021.07.002},
   journal={AI Open},
   publisher={Elsevier BV},
   author={Cartuyvels, Ruben and Spinks, Graham and Moens, Marie-Francine},
   year={2021},
   pages={143–159} }
@article{M_ller_2022,
   title={Instant neural graphics primitives with a multiresolution hash encoding},
   volume={41},
   ISSN={1557-7368},
   url={http://dx.doi.org/10.1145/3528223.3530127},
   DOI={10.1145/3528223.3530127},
   number={4},
   journal={ACM Transactions on Graphics},
   publisher={Association for Computing Machinery (ACM)},
   author={Müller, Thomas and Evans, Alex and Schied, Christoph and Keller, Alexander},
   year={2022},
   month=jul, pages={1–15} }
@misc{hu2025onlinelearningunlearning,
      title={Online Learning and Unlearning}, 
      author={Yaxi Hu and Bernhard Schölkopf and Amartya Sanyal},
      year={2025},
      eprint={2505.08557},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2505.08557}, 
}
@misc{orabona2025modernintroductiononlinelearning,
      title={A Modern Introduction to Online Learning}, 
      author={Francesco Orabona},
      year={2025},
      eprint={1912.13213},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1912.13213}, 
}
@misc{hoi2018onlinelearningcomprehensivesurvey,
      title={Online Learning: A Comprehensive Survey}, 
      author={Steven C. H. Hoi and Doyen Sahoo and Jing Lu and Peilin Zhao},
      year={2018},
      eprint={1802.02871},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1802.02871}, 
}
@book{zhang2023mathematical,
  author    = {Tong Zhang},
  title     = {Mathematical Analysis of Machine Learning Algorithms},
  publisher = {Cambridge University Press},
  year      = {2023},
  doi       = {10.1017/9781009093057},
  url       = {https://www.cambridge.org/core/books/mathematical-analysis-of-machine-learning-algorithms/EB9BABB05A5C312F19C38E5A01A5ECFC}
}
@misc{song2025languagemodelsfailintrospect,
      title={Language Models Fail to Introspect About Their Knowledge of Language}, 
      author={Siyuan Song and Jennifer Hu and Kyle Mahowald},
      year={2025},
      eprint={2503.07513},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.07513}, 
}
@misc{galke2022emergentcommunicationunderstandinghuman,
      title={Emergent Communication for Understanding Human Language Evolution: What's Missing?}, 
      author={Lukas Galke and Yoav Ram and Limor Raviv},
      year={2022},
      eprint={2204.10590},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2204.10590}, 
}
@misc{worden2025unifiedtheorylanguage,
      title={A Unified Theory of Language}, 
      author={Robert Worden},
      year={2025},
      eprint={2508.20109},
      archivePrefix={arXiv},
      primaryClass={q-bio.NC},
      url={https://arxiv.org/abs/2508.20109}, 
}
@book{grimm1819deutsche,
  author    = {Grimm, Jacob},
  title     = {Deutsche Grammatik},
  year      = {1819},
  publisher = {Bei Dieterich},
  address   = {G\"ottingen},
  note      = {Vol. 1 (other vols. published 1822--1837)},
  url       = {https://archive.org/details/deutschegrammati01grim}
}
@book{humboldt1836_language,
  author    = {Humboldt, Wilhelm von},
  title     = {On Language: The Diversity of Human Language-Structure and its Influence on the Mental Development of Mankind},
  year      = {1836},
  note      = {Originally published in German as ``\emph{\"Uber die Verschiedenheit des menschlichen Sprachbaues}''; English translations published later (e.g., Cambridge Univ.\ Press ed.)},
  publisher = {Originally published 1836; modern English ed. Cambridge Univ. Press (1988)},
  url       = {https://archive.org/details/onlanguagedivers0000humb}
}
@book{schleicher1874_compendium,
  author      = {Schleicher, August},
  title       = {A Compendium of the Comparative Grammar of the Indo-European, Sanskrit, Greek and Latin Languages},
  year        = {1874},
  translator  = {Herbert Bendall},
  publisher   = {Tr{\"u}bner \& Co.},
  address     = {London},
  url         = {https://archive.org/details/compendiumofcomp01schluoft}
}
@book{darwin1871_descent,
  author    = {Darwin, Charles},
  title     = {The Descent of Man, and Selection in Relation to Sex},
  year      = {1871},
  publisher = {John Murray},
  address   = {London},
  url       = {https://www.gutenberg.org/ebooks/2300}
}
@book{saussure1916_course,
  author      = {Saussure, Ferdinand de},
  title       = {Course in General Linguistics},
  year        = {1916},
  editor      = {Charles Bally and Albert S\'ech\'ehaye},
  note        = {Compiled from lecture notes; classic edition/English translations include Wade Baskin (1959) and Roy Harris (1983).},
  publisher   = {Philosophical Library / Duckworth (English translations)},
  url         = {https://ia600204.us.archive.org/0/items/SaussureFerdinandDeCourseInGeneralLinguistics1959/Saussure_Ferdinand_de_Course_in_General_Linguistics_1959.pdf}
}
@book{bloomfield1933_language,
  author    = {Bloomfield, Leonard},
  title     = {Language},
  year      = {1933},
  publisher = {Henry Holt and Company},
  address   = {New York},
  url       = {https://books.google.com/books/about/Language.html?id=zduwAAAAIAAJ}
}
@article{hockett1960_origin,
  author  = {Hockett, Charles F.},
  title   = {The Origin of Speech},
  journal = {Scientific American},
  year    = {1960},
  volume  = {203},
  number  = {3},
  pages   = {88--111},
  doi     = {10.1038/scientificamerican0960-88},
  url     = {https://openlab.bmcc.cuny.edu/lin100b05w/wp-content/uploads/sites/3689/2024/03/The-Origin-of-Speech.pdf}
}
@book{chomsky1957_syntactic,
  author    = {Chomsky, Noam},
  title     = {Syntactic Structures},
  year      = {1957},
  publisher = {Mouton \& Co.},
  address   = {The Hague},
  url       = {https://degruyter.com/document/doi/10.1515/9783112316009/html}
}
@book{chomsky1965_aspects,
  author    = {Chomsky, Noam},
  title     = {Aspects of the Theory of Syntax},
  year      = {1965},
  publisher = {MIT Press},
  address   = {Cambridge, MA},
  url       = {https://mitpress.mit.edu/9780262530071/aspects-of-the-theory-of-syntax/}
}
@book{goldberg1995_constructions,
  author    = {Goldberg, Adele E.},
  title     = {Constructions: A Construction Grammar Approach to Argument Structure},
  year      = {1995},
  publisher = {University of Chicago Press},
  address   = {Chicago},
  url       = {https://press.uchicago.edu/ucp/books/book/chicago/C/bo3683810.html}
}
@inproceedings{fillmore1988_mechanisms,
  author    = {Fillmore, Charles J.},
  title     = {The Mechanisms of ``Construction Grammar''},
  booktitle = {Proceedings of the Fourteenth Annual Meeting of the Berkeley Linguistics Society},
  year      = {1988},
  pages     = {35--55},
  doi       = {10.3765/bls.v14i0.1794},
  url       = {https://journals.linguisticsociety.org/proceedings/index.php/BLS/article/view/1794}
}
@book{tomasello2003_constructing,
  author    = {Tomasello, Michael},
  title     = {Constructing a Language: A Usage-Based Theory of Language Acquisition},
  year      = {2003},
  publisher = {Harvard University Press},
  address   = {Cambridge, MA},
  url       = {https://www.hup.harvard.edu/catalog.php?isbn=9780674010303}
}
@article{pinker1990_natural,
  author  = {Pinker, Steven and Bloom, Paul},
  title   = {Natural Language and Natural Selection},
  journal = {Behavioral and Brain Sciences},
  year    = {1990},
  volume  = {13},
  number  = {4},
  pages   = {707--727},
  note    = {Target article with commentaries; preprint available},
  url     = {https://stevenpinker.com/files/pinker/files/pinker_bloom_1990.pdf}
}
@book{deacon1997_symbolic,
  author    = {Deacon, Terrence W.},
  title     = {The Symbolic Species: The Co-evolution of Language and the Brain},
  year      = {1997},
  publisher = {W. W. Norton \& Company},
  address   = {New York},
  url       = {https://uberty.org/wp-content/uploads/2016/02/Terrence_W._Deacon_The_Symbolic_Species.pdf}
}
@book{bickerton1990_language_and_species,
  author    = {Bickerton, Derek},
  title     = {Language and Species},
  year      = {1990},
  publisher = {University of Chicago Press},
  address   = {Chicago},
  url       = {https://archive.org/details/languagespecies0000bick}
}
@article{kirby2001_spontaneous,
  author  = {Kirby, Simon},
  title   = {Spontaneous evolution of linguistic structure: An iterated learning model of the emergence of regularity and irregularity},
  journal = {IEEE Transactions on Evolutionary Computation},
  year    = {2001},
  volume  = {5},
  number  = {2},
  pages   = {102--110},
  doi     = {10.1109/4235.918430},
  url     = {https://www.ling.ed.ac.uk/~simon/Papers/Kirby/Spontaneous%20evolution%20of%20linguistic%20structure%20an%20iterated.pdf}
}
@book{christiansen_kirby2003_language,
  editor    = {Christiansen, Morten H. and Kirby, Simon},
  title     = {Language Evolution},
  year      = {2003},
  publisher = {Oxford University Press},
  address   = {Oxford},
  series    = {Oxford Studies in the Evolution of Language},
  isbn      = {0199244839},
  url       = {https://www.lel.ed.ac.uk/~simon/0-19-924484-7.pdf}
}
@article{nowak2001_evolution_universal_grammar,
  author  = {Nowak, Martin A. and Komarova, Natalia L. and Niyogi, Partha},
  title   = {Evolution of universal grammar},
  journal = {Science},
  year    = {2001},
  volume  = {291},
  number  = {5501},
  pages   = {114--118},
  doi     = {10.1126/science.291.5501.114},
  url     = {https://pubmed.ncbi.nlm.nih.gov/11141560/}
}
@article{evans2009_myth_language_universals,
  author  = {Evans, Nicholas and Levinson, Stephen C.},
  title   = {The myth of language universals: Language diversity and its importance for cognitive science},
  journal = {Behavioral and Brain Sciences},
  year    = {2009},
  volume  = {32},
  number  = {5},
  pages   = {429--492},
  doi     = {10.1017/S0140525X0999094X},
  url     = {https://cognitionandculture.net/wp-content/uploads/Evans-Levinson-BBS-2009-2.pdf}
}
@article{christiansen2008_language_shaped_by_brain,
  author  = {Christiansen, Morten H. and Chater, Nick},
  title   = {Language as shaped by the brain},
  journal = {Behavioral and Brain Sciences},
  year    = {2008},
  volume  = {31},
  number  = {5},
  pages   = {489--558},
  doi     = {10.1017/S0140525X08004998},
  url     = {https://greenfieldlab.psych.ucla.edu/wp-content/uploads/sites/168/2019/06/14-S0140525X08005141a.pdf}
}
@article{hauser2002_faculty,
  author  = {Hauser, Marc D. and Chomsky, Noam and Fitch, W. Tecumseh},
  title   = {The faculty of language: What is it, who has it, and how did it evolve?},
  journal = {Science},
  year    = {2002},
  volume  = {298},
  number  = {5598},
  pages   = {1569--1579},
  doi     = {10.1126/science.298.5598.1569},
  url     = {https://web.stanford.edu/class/linguist197a/hauser.pdf}
}
@incollection{lighthill1973_artificial,
  author    = {Lighthill, J.},
  title     = {Artificial Intelligence: A General Survey},
  booktitle = {Artificial Intelligence: A Paper Symposium},
  editor    = {Lighthill, J. and Sutherland, N.~S. and Needham, R.~M. and Longuet-Higgins, H.~C. and Michie, D.},
  publisher = {Science Research Council of Great Britain},
  address   = {London},
  year      = {1973},
  pages     = {1--21},
  url       = {https://www.aiai.ed.ac.uk/events/lighthill1973/lighthill.pdf}
}
@misc{jelodar2018latentdirichletallocationlda,
      title={Latent Dirichlet Allocation (LDA) and Topic modeling: models, applications, a survey}, 
      author={Hamed Jelodar and Yongli Wang and Chi Yuan and Xia Feng and Xiahui Jiang and Yanchao Li and Liang Zhao},
      year={2018},
      eprint={1711.04305},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/1711.04305}, 
}
% Recurrent Neural Networks - Foundational Papers
@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={Nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group}
}

@article{elman1990finding,
  title={Finding structure in time},
  author={Elman, Jeffrey L},
  journal={Cognitive Science},
  volume={14},
  number={2},
  pages={179--211},
  year={1990},
  publisher={Wiley Online Library}
}

@article{jordan1997serial,
  title={Serial order: A parallel distributed processing approach},
  author={Jordan, Michael I},
  journal={Advances in Psychology},
  volume={121},
  pages={471--495},
  year={1997},
  publisher={Elsevier}
}

% LSTM - Long Short-Term Memory
@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural Computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@article{gers2000learning,
  title={Learning to forget: Continual prediction with LSTM},
  author={Gers, Felix A and Schmidhuber, J{\"u}rgen and Cummins, Fred},
  journal={Neural Computation},
  volume={12},
  number={10},
  pages={2451--2471},
  year={2000},
  publisher={MIT Press}
}

% GRU - Gated Recurrent Unit
@inproceedings{cho2014learning,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  booktitle={Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={1724--1734},
  year={2014}
}

@article{chung2014empirical,
  title={Empirical evaluation of gated recurrent neural networks on sequence modeling},
  author={Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1412.3555},
  year={2014}
}

% Bidirectional RNNs
@article{schuster1997bidirectional,
  title={Bidirectional recurrent neural networks},
  author={Schuster, Mike and Paliwal, Kuldip K},
  journal={IEEE Transactions on Signal Processing},
  volume={45},
  number={11},
  pages={2673--2681},
  year={1997},
  publisher={IEEE}
}

% Sequence-to-Sequence Models
@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={Advances in Neural Information Processing Systems},
  volume={27},
  pages={3104--3112},
  year={2014}
}

% Attention Mechanism
@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@inproceedings{luong2015effective,
  title={Effective approaches to attention-based neural machine translation},
  author={Luong, Minh-Thang and Pham, Huynh and Manning, Christopher D},
  booktitle={Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  pages={1412--1421},
  year={2015}
}

% Transformer Architecture

% Self-Attention
@article{cheng2016long,
  title={Long short-term memory-networks for machine reading},
  author={Cheng, Jianpeng and Dong, Li and Lapata, Mirella},
  journal={arXiv preprint arXiv:1601.06733},
  year={2016}
}

@inproceedings{parikh2016decomposable,
  title={A decomposable attention model for natural language inference},
  author={Parikh, Ankur P and T{\"a}ckstr{\"o}m, Oscar and Das, Dipanjan and Uszkoreit, Jakob},
  booktitle={Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  pages={2249--2255},
  year={2016}
}

% Notable Survey Papers
@article{lipton2015critical,
  title={A critical review of recurrent neural networks for sequence learning},
  author={Lipton, Zachary C and Berkowitz, John and Elkan, Charles},
  journal={arXiv preprint arXiv:1506.00019},
  year={2015}
}

@article{graves2012supervised,
  title={Supervised sequence labelling with recurrent neural networks},
  author={Graves, Alex},
  journal={Studies in Computational Intelligence},
  volume={385},
  year={2012},
  publisher={Springer}
}
@misc{chen2019neuralordinarydifferentialequations,
      title={Neural Ordinary Differential Equations}, 
      author={Ricky T. Q. Chen and Yulia Rubanova and Jesse Bettencourt and David Duvenaud},
      year={2019},
      eprint={1806.07366},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1806.07366}, 
}
@misc{kaplan2020scalinglawsneurallanguage,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.08361}, 
}
@misc{sorscher2023neuralscalinglawsbeating,
      title={Beyond neural scaling laws: beating power law scaling via data pruning}, 
      author={Ben Sorscher and Robert Geirhos and Shashank Shekhar and Surya Ganguli and Ari S. Morcos},
      year={2023},
      eprint={2206.14486},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2206.14486}, 
}
@inproceedings{ivgi-etal-2022-scaling,
    title = "Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments",
    author = "Ivgi, Maor  and
      Carmon, Yair  and
      Berant, Jonathan",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.544/",
    doi = "10.18653/v1/2022.findings-emnlp.544",
    pages = "7354--7371",
    abstract = "Neural scaling laws define a predictable relationship between a model{'}s parameter count and its performance after training in the form of a power law. However, most research to date has not explicitly investigated whether scaling laws can be used to accelerate model development. In this work, we perform such an empirical investigation across a wide range of language understanding tasks, starting from models with as few as 10K parameters, and evaluate downstream performance across 9 language understanding tasks.We find that scaling laws emerge at finetuning time in some NLP tasks, and that they can also be exploited for debugging convergence when training large models. Moreover, for tasks where scaling laws exist, they can be used to predict the performance of larger models, which enables effective model selection. However, revealing scaling lawsrequires careful hyperparameter tuning and multiple runs for the purpose of uncertainty estimation, which incurs additional overhead, partially offsetting the computational benefits."
}
@misc{su2024unravelingmysteryscalinglaws,
      title={Unraveling the Mystery of Scaling Laws: Part I}, 
      author={Hui Su and Zhi Tian and Xiaoyu Shen and Xunliang Cai},
      year={2024},
      eprint={2403.06563},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.06563}, 
}
@misc{lee2025neuralscalinglawsleading,
      title={Are neural scaling laws leading quantum chemistry astray?}, 
      author={Siwoo Lee and Adji Bousso Dieng},
      year={2025},
      eprint={2509.26397},
      archivePrefix={arXiv},
      primaryClass={physics.chem-ph},
      url={https://arxiv.org/abs/2509.26397}, 
}
@article{doi:10.1073/pnas.2311878121,
author = {Yasaman Bahri  and Ethan Dyer  and Jared Kaplan  and Jaehoon Lee  and Utkarsh Sharma },
title = {Explaining neural scaling laws},
journal = {Proceedings of the National Academy of Sciences},
volume = {121},
number = {27},
pages = {e2311878121},
year = {2024},
doi = {10.1073/pnas.2311878121},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2311878121},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2311878121},
abstract = {The population loss of trained deep neural networks has been empirically observed to improve as a power law in a variety of large models and datasets. We investigate the origins behind such “scaling laws” and provide a taxonomy for different scaling regimes. Our findings are based on derivations in linear random feature models—which, in addition to being a simple fruitful model, also describe the wide network limit of deep neural networks. We further formulate and verify aspects of scaling based on smoothness in interpolating a data manifold. We support our theory with empirical results in realistic settings. Our work provides insights into scaling laws and bridges the large gap between theory and experiment in modern deep learning. The population loss of trained deep neural networks often follows precise power-law scaling relations with either the size of the training dataset or the number of parameters in the network. We propose a theory that explains the origins of and connects these scaling laws. We identify variance-limited and resolution-limited scaling behavior for both dataset and model size, for a total of four scaling regimes. The variance-limited scaling follows simply from the existence of a well-behaved infinite data or infinite width limit, while the resolution-limited regime can be explained by positing that models are effectively resolving a smooth data manifold. In the large width limit, this can be equivalently obtained from the spectrum of certain kernels, and we present evidence that large width and large dataset resolution-limited scaling exponents are related by a duality. We exhibit all four scaling regimes in the controlled setting of large random feature and pretrained models and test the predictions empirically on a range of standard architectures and datasets. We also observe several empirical relationships between datasets and scaling exponents under modifications of task and architecture aspect ratio. Our work provides a taxonomy for classifying different scaling regimes, underscores that there can be different mechanisms driving improvements in loss, and lends insight into the microscopic origin and relationships between scaling exponents.}}
@misc{power2022grokking,
  title        = {Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets},
  author       = {A. Power and Y. Burda and H. Edwards and I. Babuschkin and V. Misra},
  year         = {2022},
  eprint       = {2201.02177},
  archivePrefix= {arXiv},
  primaryClass = {cs.LG},
  url          = {https://arxiv.org/abs/2201.02177}
}
@inproceedings{szegedy2013intriguing,
  title = {Intriguing properties of neural networks},
  author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  booktitle = {International Conference on Learning Representations (ICLR) Workshop},
  year = {2014},
  note = {preprint arXiv:1312.6199},
  url = {https://arxiv.org/abs/1312.6199}
}
@article{goodfellow2014explaining,
  title = {Explaining and harnessing adversarial examples},
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  journal = {arXiv preprint},
  year = {2014},
  eprint = {1412.6572},
  url = {https://arxiv.org/abs/1412.6572}
}
@misc{carlini2017evaluatingrobustnessneuralnetworks,
      title={Towards Evaluating the Robustness of Neural Networks}, 
      author={Nicholas Carlini and David Wagner},
      year={2017},
      eprint={1608.04644},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/1608.04644}, 
}
@inproceedings{arpit2017closer,
  title = {A closer look at memorization in deep networks},
  author = {Arpit, Devansh and Jastrzebski, Stanislaw and Ballas, Nicolas and Krueger, David and Bengio, E and Serban, I. and others},
  booktitle = {International Conference on Machine Learning (ICML)},
  year = {2017},
  note = {arXiv:1706.05394},
  url = {https://arxiv.org/abs/1706.05394}
}
@article{kirkpatrick2017overcoming,
  title = {Overcoming catastrophic forgetting in neural networks},
  author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, K and others},
  journal = {Proceedings of the National Academy of Sciences (PNAS)},
  year = {2017},
  volume = {114},
  number = {13},
  pages = {3521--3526},
  doi = {10.1073/pnas.1611835114},
  url = {https://www.pnas.org/content/114/13/3521}
}
@misc{hsieh2024comprehensiveguideexplainableai,
      title={A Comprehensive Guide to Explainable AI: From Classical Models to LLMs}, 
      author={Weiche Hsieh and Ziqian Bi and Chuanqi Jiang and Junyu Liu and Benji Peng and Sen Zhang and Xuanhe Pan and Jiawei Xu and Jinlang Wang and Keyu Chen and Pohsun Feng and Yizhu Wen and Xinyuan Song and Tianyang Wang and Ming Liu and Junjie Yang and Ming Li and Bowen Jing and Jintao Ren and Junhao Song and Hong-Ming Tseng and Yichao Zhang and Lawrence K. Q. Yan def:SMLPand Qian Niu and Silin Chen and Yunze Wang and Chia Xin Liang},
      year={2024},
      eprint={2412.00800},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.00800}, 
}
@misc{cogprints553, title = {A Refutation of Penrose's Godelian Case Against Artificial Intelligence}, author = {Selmer Bringsjord}, year = {2000}, keywords = {Godel's Incompleteness Theorem, Penrose, Artificial Intelligence, Minds, Machines}, url = {http://cogprints.org/553/}, abstract = {Having, as it is generally agreed, failed to destroy the computational conception of mind with the G{$\backslash$}"\{o\}delian attack he articulated in his \{{$\backslash$}em The Emperor's New Mind\}, Penrose has returned, armed with a more elaborate and more fastidious G{$\backslash$}"\{o\}delian case, expressed in and 3 of his \{{$\backslash$}em Shadows of the Mind\}. The core argument in these chapters is enthymematic, and when formalized, a remarkable number of technical glitches come to light. Over and above these defects, the argument, at best, is an instance of either the fallacy of denying the antecedent, the fallacy of \{{$\backslash$}em petitio principii\}, or the fallacy of equivocation. More recently, writing in response to his critics in the electronic journal \{{$\backslash$}em Psyche\}, Penrose has offered a G{$\backslash$}"\{o\}delian case designed to improve on the version presented in \{{$\backslash$}em SOTM\}. But this version is yet again another failure. In falling prey to the errors we uncover, Penrose's new G{$\backslash$}"\{o\}delian case is unmasked as the same confused refrain J.R. Lucas initiated 35 years ago.} }
@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of Control, Signals and Systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  publisher={Springer},
  doi={10.1007/BF02551274}
}
@article{hornik1989multilayer,
  title={Multilayer feedforward networks are universal approximators},
  author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal={Neural Networks},
  volume={2},
  number={5},
  pages={359--366},
  year={1989},
  publisher={Elsevier},
  doi={10.1016/0893-6080(89)90020-8}
}
@article{leshno1993multilayer,
  title={Multilayer feedforward networks with a nonpolynomial activation function can approximate any function},
  author={Leshno, Moshe and Lin, Vladimir Ya and Pinkus, Allan and Schocken, Shimon},
  journal={Neural Networks},
  volume={6},
  number={6},
  pages={861--867},
  year={1993},
  publisher={Elsevier},
  doi={10.1016/S0893-6080(05)80131-5}
}
@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural Networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier},
  doi={10.1016/0893-6080(91)90009-T}
}
@article{luo1992convergence,
  title={On the convergence of the coordinate descent method for convex differentiable minimization},
  author={Luo, Zhi-Quan and Tseng, Paul},
  journal={Journal of Optimization Theory and Applications},
  volume={72},
  number={1},
  pages={7--35},
  year={1992},
  publisher={Springer},
  doi={10.1007/BF00939948}
}

@article{tseng2001convergence,
  title={Convergence of a block coordinate descent method for nondifferentiable minimization},
  author={Tseng, Paul},
  journal={Journal of Optimization Theory and Applications},
  volume={109},
  number={3},
  pages={475--494},
  year={2001},
  publisher={Springer},
  doi={10.1007/s10957-001-0006-4}
}

@article{friedman2007pathwise,
  title={Pathwise coordinate optimization},
  author={Friedman, Jerome and Hastie, Trevor and H{\"o}fling, Holger and Tibshirani, Robert},
  journal={The Annals of Applied Statistics},
  volume={1},
  number={2},
  pages={302--332},
  year={2007},
  publisher={Institute of Mathematical Statistics},
  doi={10.1214/07-AOAS131}
}

@article{wright2015coordinate,
  title={Coordinate descent algorithms},
  author={Wright, Stephen J},
  journal={Mathematical Programming},
  volume={151},
  number={1},
  pages={3--34},
  year={2015},
  publisher={Springer},
  doi={10.1007/s10107-015-0892-3}
}

@article{tseng2009coordinate,
  title={A coordinate gradient descent method for nonsmooth separable minimization},
  author={Tseng, Paul and Yun, Sangwoon},
  journal={Mathematical Programming},
  volume={117},
  number={1},
  pages={387--423},
  year={2009},
  publisher={Springer},
  doi={10.1007/s10107-007-0170-0}
}
@misc{alon2021bottleneckgraphneuralnetworks,
      title={On the Bottleneck of Graph Neural Networks and its Practical Implications}, 
      author={Uri Alon and Eran Yahav},
      year={2021},
      eprint={2006.05205},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.05205}, 
}
@inproceedings{NEURIPS2019_bb04af0f,
 author = {Maron, Haggai and Ben-Hamu, Heli and Serviansky, Hadar and Lipman, Yaron},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Provably Powerful Graph Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/bb04af0f7ecaee4aae62035497da1387-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{kearns1998efficient,
  title = {Efficient noise-tolerant learning from statistical queries},
  author = {Kearns, Michael J.},
  journal = {Journal of the ACM},
  volume = {45},
  number = {6},
  pages = {983--1006},
  year = {1998},
  publisher = {ACM}
}

@article{blum2003noise,
  title = {Noise-tolerant learning, the parity problem, and the statistical query model},
  author = {Blum, Avrim and Kalai, Adam Tauman and Wasserman, Hal},
  journal = {Journal of the ACM},
  volume = {50},
  number = {4},
  pages = {506--519},
  year = {2003},
  publisher = {ACM},
  doi = {10.1145/792538.792543}
}

@article{garg2021memory,
  title = {Memory-Sample Lower Bounds for Learning Parity with Noise},
  author = {Garg, Sumegha and Kothari, Pravesh K. and Liu, Pengda and Raz, Ran},
  journal = {arXiv preprint arXiv:2107.02320},
  year = {2021},
  url = {https://arxiv.org/abs/2107.02320}
}

@inproceedings{barak2016nearly,
  title = {A Nearly Tight Sum-of-Squares Lower Bound for the Planted Clique Problem},
  author = {Barak, Boaz and Hopkins, Samuel B. and Kelner, Jonathan and Kothari, Pravesh and Moitra, Ankur and Potechin, Aaron},
  booktitle = {Proceedings of the 57th Annual IEEE Symposium on Foundations of Computer Science (FOCS)},
  year = {2016},
  pages = {428--437},
  doi = {10.1109/FOCS.2016.52}
}

@article{blum1992training,
  title = {Training a 3-node neural network is NP-complete},
  author = {Blum, Avrim L. and Rivest, Ronald L.},
  journal = {Neural Networks},
  volume = {5},
  number = {1},
  pages = {117--127},
  year = {1992},
  publisher = {Elsevier}
}

@inproceedings{abrahamsen2021existential,
  title = {Training Neural Networks is {$\exists\mathbb{R}$}-complete},
  author = {Abrahamsen, Mikkel and Kleist, Linda and Miltzow, Tillmann},
  booktitle = {Proceedings of NeurIPS 2021},
  year = {2021},
  note = {arXiv:2102.09798},
  url = {https://arxiv.org/abs/2102.09798}
}

@inproceedings{eldan2016power,
  title = {The Power of Depth for Feedforward Neural Networks},
  author = {Eldan, Ronen and Shamir, Ohad},
  booktitle = {Proceedings of the 29th Annual Conference on Learning Theory (COLT)},
  series = {Proceedings of Machine Learning Research},
  volume = {49},
  pages = {907--940},
  year = {2016},
  url = {https://proceedings.mlr.press/v49/eldan16.html}
}

@inproceedings{telgarsky2016benefits,
  title = {Benefits of Depth in Neural Networks},
  author = {Telgarsky, Matus},
  booktitle = {Proceedings of COLT 2016},
  year = {2016},
  url = {https://arxiv.org/abs/1602.04485}
}

@article{siegelmann1991turing,
  title = {Turing Computability with Neural Nets},
  author = {Siegelmann, Hava T. and Sontag, Eduardo D.},
  journal = {Applied Mathematics Letters},
  volume = {4},
  number = {6},
  pages = {77--80},
  year = {1991},
  doi = {10.1016/0893-9659(91)90080-F},
  url = {https://www.sontaglab.org/FTPDIR/aml-turing.pdf}
}

@inproceedings{weiss2018practical,
  title = {On the Practical Computational Power of Finite Precision RNNs for Language Recognition},
  author = {Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle = {Proceedings of ACL (short papers) / arXiv},
  year = {2018},
  note = {arXiv:1805.04908},
  url = {https://arxiv.org/abs/1805.04908}
}
@article{wolpert1997nofree,
  title = {No Free Lunch Theorems for Optimization},
  author = {Wolpert, David H. and Macready, William G.},
  journal = {IEEE Transactions on Evolutionary Computation},
  volume = {1},
  number = {1},
  pages = {67--82},
  year = {1997},
  doi = {10.1109/4235.585893},
  url = {https://www.cs.ubc.ca/~hutter/earg/papers07/00585893.pdf}
}

